{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building Chatbots with AWS Bedrock: Hands-on Guide\n",
        "\n",
        "Hello everyone! üëã\n",
        "\n",
        "In this guide, I'll explain how to build a chatbot on AWS without any headaches.\n",
        "\n",
        "## Project Context\n",
        "This guide documents my exploration of AWS Bedrock for chatbot development within an existing AWS infrastructure. No vendor comparisons ‚Äî just practical implementation steps. This guide based by official amazon-bedrock-workshop repository."
      ],
      "metadata": {
        "id": "rs6tWkUrtvB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setting Up: First API Call\n",
        "Goal: Initialize a model and make a basic API call."
      ],
      "metadata": {
        "id": "xZECL-qauh7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyQsDl8hwrUi",
        "outputId": "dd32fccd-3ccb-4a2f-b7e2-fbb3be49a641"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.42.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting botocore<1.42.0,>=1.41.6 (from boto3)\n",
            "  Downloading botocore-1.41.6-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3)\n",
            "  Downloading s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.42.0,>=1.41.6->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.42.0,>=1.41.6->boto3) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.42.0,>=1.41.6->boto3) (1.17.0)\n",
            "Downloading boto3-1.42.0-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.41.6-py3-none-any.whl (14.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.42.0 botocore-1.41.6 jmespath-1.0.1 s3transfer-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import botocore\n",
        "import boto3\n",
        "from IPython.display import display, Markdown\n",
        "import time"
      ],
      "metadata": {
        "id": "0DP8P2Tiw9cD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start with the region initialization."
      ],
      "metadata": {
        "id": "-drMtyqqxDHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a file with AWS credentials.\n",
        "\n",
        "```\n",
        "aws configure\n",
        "```\n",
        "\n",
        "Or create a file manually here\n",
        "\n",
        "```\n",
        "Linux/MacOS: ~/.aws/credentials\n",
        "Windows: C:\\Users\\YOUR_USER\\.aws\\credentials\n",
        "```\n",
        "\n",
        "```\n",
        "[default]\n",
        "aws_access_key_id = YOUR_ACCESS_KEY_ID\n",
        "aws_secret_access_key = YOUR_SECRET_ACCESS_KEY\n",
        "region = us-east-1  # or yours region\n",
        "```\n"
      ],
      "metadata": {
        "id": "K9EyK3LSfYY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modify your code to load settings"
      ],
      "metadata": {
        "id": "yAAXJSqDgjti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Using boto3 with a profile\n",
        "\n",
        "```\n",
        "import boto3\n",
        "import json\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "# –£–∫–∞–∂–∏—Ç–µ –ø—Ä–æ—Ñ–∏–ª—å —è–≤–Ω–æ\n",
        "session = boto3.Session(\n",
        "    profile_name='default',\n",
        "    region_name='us-east-1'  \n",
        "```\n",
        "\n",
        "### 2. Load from JSON\n",
        "\n",
        "```\n",
        "{\n",
        "    \"aws_access_key_id\": \"YOUR_ACCESS_KEY_ID\",\n",
        "    \"aws_secret_access_key\": \"YOUR_SECRET_ACCESS_KEY\",\n",
        "    \"region\": \"us-east-1\"\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "PLTKjw6WhIxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = '/content/aws_config.js'"
      ],
      "metadata": {
        "id": "RfRGxHmUxs1s"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import json\n",
        "import os\n",
        "\n",
        "def load_aws_credentials(json_file_path):\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        credentials = json.load(f)\n",
        "\n",
        "    os.environ['AWS_ACCESS_KEY_ID'] = credentials['aws_access_key_id']\n",
        "    os.environ['AWS_SECRET_ACCESS_KEY'] = credentials['aws_secret_access_key']\n",
        "    os.environ['AWS_DEFAULT_REGION'] = credentials.get('region', 'us-east-1')\n",
        "\n",
        "    return credentials\n",
        "\n",
        "load_aws_credentials('aws_credentials.json')\n",
        "\n",
        "bedrock = boto3.client('bedrock-runtime')"
      ],
      "metadata": {
        "id": "GIXCGRJWgoI8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Bedrock client\n",
        "session = boto3.session.Session()\n",
        "region = session.region_name\n",
        "bedrock = boto3.client(service_name='bedrock-runtime', region_name=\"us-east-1\")"
      ],
      "metadata": {
        "id": "BO5Ar5rzwpll"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS = {\n",
        "    \"Claude 3.7 Sonnet\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
        "    \"Amazon Nova Pro\": \"us.amazon.nova-pro-v1:0\",\n",
        "    \"Amazon Nova Micro\": \"us.amazon.nova-micro-v1:0\",\n",
        "}"
      ],
      "metadata": {
        "id": "KqI--BsZ0t8k"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to display model responses in a more readable format\n",
        "def display_response(response, model_name=None):\n",
        "    if model_name:\n",
        "        display(Markdown(f\"### Response from {model_name}\"))\n",
        "    display(Markdown(response))\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "Bt8O4CYyc8f3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Text Summarization with Foundation Models (basic Invoke API)"
      ],
      "metadata": {
        "id": "S2NoM_c_dCCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_summarize = \"\"\"\n",
        "AWS took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, \\\n",
        "a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. \\\n",
        "Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, \\\n",
        "democratizing access for all builders. Bedrock will offer the ability to access a range of powerful FMs \\\n",
        "for text and images‚Äîincluding Amazons Titan FMs, which consist of two new LLMs we're also announcing \\\n",
        "today‚Äîthrough a scalable, reliable, and secure AWS managed service. With Bedrock's serverless experience, \\\n",
        "customers can easily find the right model for what they're trying to get done, get started quickly, privately \\\n",
        "customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS \\\n",
        "tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations \\\n",
        "with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale).\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QRzn1cqqdEeA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Each model family has its own distinct request and response format, you'll need to craft specific JSON payloads tailored to each model"
      ],
      "metadata": {
        "id": "hM517UUqdUvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Please provide a summary of the following text. Do not add any information that is not mentioned in the text below.\n",
        "<text>\n",
        "{text_to_summarize}\n",
        "</text>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Yd_Wa9npddFD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create request body for Claude 3.7 Sonnet\n",
        "claude_body = json.dumps({\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "    \"max_tokens\": 1000,\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_p\": 0.9,\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
        "        }\n",
        "    ],\n",
        "})\n",
        "\n",
        "# Send request to Claude 3.7 Sonnet\n",
        "try:\n",
        "    response = bedrock.invoke_model(\n",
        "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
        "        body=claude_body,\n",
        "        accept=\"application/json\",\n",
        "        contentType=\"application/json\"\n",
        "    )\n",
        "    response_body = json.loads(response.get('body').read())\n",
        "\n",
        "    # Extract and display the response text\n",
        "    claude_summary = response_body[\"content\"][0][\"text\"]\n",
        "    display_response(claude_summary, \"Claude 3.7 Sonnet (Invoke Model API)\")\n",
        "\n",
        "except botocore.exceptions.ClientError as error:\n",
        "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
        "        print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
        "            \\nTo troubleshoot this issue please refer to the following resources.\\\n",
        "            \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
        "            \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
        "    else:\n",
        "        raise error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "M9Njcc7seX1M",
        "outputId": "4c5f65ef-8d14-4293-bd94-bd86a86397d2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Response from Claude 3.7 Sonnet (Invoke Model API)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Summary\n\nAmazon announced Amazon Bedrock, a new service that provides API access to foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon. Bedrock aims to democratize access to generative AI by offering a simple way for customers to build and scale applications using these models. The service includes text and image models, including Amazon's new Titan LLMs. As a serverless AWS managed service, Bedrock allows customers to find appropriate models, get started quickly, customize models with their own data, and integrate them into applications using familiar AWS tools without managing infrastructure. The service integrates with Amazon SageMaker features like Experiments and Pipelines."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Text Summarization using the Converse API\n",
        "\n",
        "Invoke Model API has several limitations:\n",
        "- it uses different request/response formats for each model family;\n",
        "- there is no built-in support for multi-turn conversations;\n",
        "- it requires custom handling for different model capabilities\n",
        "\n",
        "Converse API addresses these limitations by providing a unified interface."
      ],
      "metadata": {
        "id": "RAsUb7C2yL7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converse_request = {\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"text\": f\"Please provide a concise summary of the following text in 2-3 sentences. Text to summarize: {text_to_summarize}\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    \"inferenceConfig\": {\n",
        "        \"temperature\": 0.4,\n",
        "        \"topP\": 0.9,\n",
        "        \"maxTokens\": 500\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "e8UwClVtyylS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    response = bedrock.converse(\n",
        "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
        "        messages=converse_request[\"messages\"],\n",
        "        inferenceConfig=converse_request[\"inferenceConfig\"]\n",
        "    )\n",
        "\n",
        "    # Extract the model's response\n",
        "    claude_converse_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
        "    display_response(claude_converse_response, \"Claude 3.7 Sonnet (Converse API)\")\n",
        "except botocore.exceptions.ClientError as error:\n",
        "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
        "        print(f\"\\x1b[41m{error.response['Error']['Code']}: {error.response['Error']['Message']}\\x1b[0m\")\n",
        "        print(\"Please ensure you have the necessary permissions for Amazon Bedrock.\")\n",
        "    else:\n",
        "        raise error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "w4eBZ5Jt5oa9",
        "outputId": "e8ae3822-135f-4dff-e9a1-fb403d84461f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Response from Claude 3.7 Sonnet (Converse API)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "AWS has launched Amazon Bedrock, a service providing API access to foundation models from AI21 Labs, Anthropic, Stability AI, and Amazon's own Titan models for text and image generation. Bedrock simplifies generative AI application development through a serverless experience, allowing customers to easily find, customize, and deploy models using familiar AWS tools without managing infrastructure."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Converse API\n",
        "\n",
        "Use the [Converse](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) or [ConverseStream](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html) API operations to send messages to a model. While you could use the standard [InvokeModel](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) calls for chat applications, AWS recommends the Converse API instead. It offers a single, consistent interface that works across all Bedrock models supporting chat, letting you write your code once and reuse it with different models.\n",
        "\n",
        "If a specific model has its own unique settings, the Converse API allows you to pass those as dedicated parameters. You can use this API to build conversational applications, like a chatbot that maintains a multi-turn dialogue. This is perfect for creating a custom assistant‚Äîfor example, one with a specific persona, like a helpful tech support agent.\n",
        "\n",
        "The Converse API also supports other core Bedrock features, including [tool](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html) use and content [guardrails](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-use-converse-api.html).\n",
        "\n",
        "The list of [all Amazon Bedrock models that support messages](https://docs.aws.amazon.com/bedrock/latest/userguide/converse-api.html).\n",
        "Converse Documentation"
      ],
      "metadata": {
        "id": "M2oi3Vjf5v6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "{\n",
        "  \"modelId\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\", // Required: Model identifier\n",
        "  \n",
        "  \"messages\": [ // Required: Conversation history\n",
        "    {\n",
        "      \"role\": \"user\", // Who sent the message\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"text\": \"Your prompt or message here\" // Message content\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ],\n",
        "  \n",
        "  \"system\": [ // Optional: System instructions\n",
        "    {\n",
        "      \"text\": \"You are a helpful AI assistant.\"\n",
        "    }\n",
        "  ],\n",
        "  \n",
        "  \"inferenceConfig\": { // Optional: Inference parameters\n",
        "    \"temperature\": 0.7, // Randomness (0.0-1.0)\n",
        "    \"topP\": 0.9, // Diversity control (0.0-1.0)\n",
        "    \"maxTokens\": 2000, // Maximum response length\n",
        "    \"stopSequences\": [] // Stop generation triggers\n",
        "  },\n",
        "  \n",
        "  \"toolConfig\": { // Optional: Function calling setup\n",
        "    \"tools\": [],\n",
        "    \"toolChoice\": {\n",
        "      \"auto\": {} // Let model decide when to use tools\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "0SGPDBt38OM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Easily switch between models\n",
        "\n",
        "One of the biggest advantages of the Converse API is the ability to easily switch between models using the exact same request format."
      ],
      "metadata": {
        "id": "BxcE5a4Z8W53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call different models with the same converse request\n",
        "results = {}\n",
        "for model_name, model_id in MODELS.items(): # looping over all models defined above\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            response = bedrock.converse(\n",
        "                modelId=model_id,\n",
        "                messages=converse_request[\"messages\"],\n",
        "                inferenceConfig=converse_request[\"inferenceConfig\"] if \"inferenceConfig\" in converse_request else None\n",
        "            )\n",
        "            end_time = time.time()\n",
        "\n",
        "            # Extract the model's response using the correct structure\n",
        "            model_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
        "            response_time = round(end_time - start_time, 2)\n",
        "\n",
        "            results[model_name] = {\n",
        "                \"response\": model_response,\n",
        "                \"time\": response_time\n",
        "            }\n",
        "\n",
        "            print(f\"‚úÖ Successfully called {model_name} (took {response_time} seconds)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error calling {model_name}: {str(e)}\")\n",
        "            results[model_name] = {\n",
        "                \"response\": f\"Error: {str(e)}\",\n",
        "                \"time\": None\n",
        "            }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNBPPybA8n4C",
        "outputId": "cecfc84b-3e33-4852-a2cf-ffb5aa32de3d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully called Claude 3.7 Sonnet (took 2.64 seconds)\n",
            "‚úÖ Successfully called Amazon Nova Pro (took 2.11 seconds)\n",
            "‚úÖ Successfully called Amazon Nova Micro (took 0.77 seconds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results in a formatted way\n",
        "for model_name, result in results.items():\n",
        "    if \"Error\" not in result[\"response\"]:\n",
        "        display(Markdown(f\"### {model_name} (took {result['time']} seconds)\"))\n",
        "        display(Markdown(result[\"response\"]))\n",
        "        print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "_dztT-hd8uxe",
        "outputId": "b67fe680-b874-4e66-8448-73de0ad8fe52"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Claude 3.7 Sonnet (took 2.64 seconds)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "AWS has launched Amazon Bedrock, a new service providing API access to foundation models (FMs) from various AI companies, including Amazon's own Titan LLMs. The service democratizes generative AI by offering a serverless experience where customers can easily find, customize, and deploy text and image models without managing infrastructure. Bedrock integrates with existing AWS tools, allowing users to test different models and manage their FMs at scale."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Amazon Nova Pro (took 2.11 seconds)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "AWS has launched Amazon Bedrock, a new service that provides easy access to Foundation Models (FMs) from various providers like AI21 Labs, Anthropic, Stability AI, and Amazon via an API, enabling customers to build and scale generative AI applications. Bedrock offers a serverless experience with scalable, reliable, and secure access to a range of powerful FMs for text and images, allowing users to customize, integrate, and deploy models using familiar AWS tools without managing infrastructure."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Amazon Nova Micro (took 0.77 seconds)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "AWS has launched Amazon Bedrock, a new service that provides easy access to generative AI models from AI21 Labs, Anthropic, Stability AI, and Amazon via an API, enabling developers to quickly build and scale AI-based applications without managing infrastructure. Bedrock offers scalable, secure access to a range of powerful models for text and images, including Amazon's new Titan models, and integrates seamlessly with AWS tools like SageMaker."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Cross-Regional Inference in Amazon Bedrock\n",
        "\n",
        "Amazon Bedrock offers Cross-Regional Inference which automatically selects the optimal AWS Region within your geography to process your inference requests.\n",
        "\n",
        "To use Cross-Regional Inference, you simply need to specify a cross-region inference profile as the modelId when making a request. Cross-region inference profiles are identified by including a region prefix (e.g., us. or eu.) before the model name.\n",
        "\n",
        "```\n",
        "{\n",
        "    \"Amazon Nova Pro\": \"amazon.nova-pro-v1:0\",  # Regular model ID\n",
        "    \"Amazon Nova Pro (CRIS)\": \"us.amazon.nova-pro-v1:0\"  # Cross-regional model ID\n",
        "}\n",
        "```\n",
        "\n",
        "```\n",
        "# Regular model invocation (standard region)\n",
        "standard_response = bedrock.converse(\n",
        "    modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Standard model ID\n",
        "    messages=converse_request[\"messages\"]\n",
        ")\n",
        "\n",
        "# Cross-region inference (note the \"us.\" prefix)\n",
        "cris_response = bedrock.converse(\n",
        "    modelId=\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Cross-region model ID with regional prefix\n",
        "    messages=converse_request[\"messages\"]\n",
        ")\n",
        "\n",
        "# Print responses\n",
        "print(\"Standard response:\", standard_response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
        "print(\"Cross-region response:\", cris_response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
        "```"
      ],
      "metadata": {
        "id": "70X_925-9ulr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Multi-turn Conversations"
      ],
      "metadata": {
        "id": "qJFD8Hu4-et7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a multi-turn conversation with Converse API\n",
        "multi_turn_messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"text\": f\"Please summarize this text: {text_to_summarize}\"}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": [{\"text\": results[\"Claude 3.7 Sonnet\"][\"response\"]}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"text\": \"Can you make this summary even shorter, just 1 sentence?\"}]\n",
        "    }\n",
        "]\n",
        "\n",
        "try:\n",
        "    response = bedrock.converse(\n",
        "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
        "        messages=multi_turn_messages,\n",
        "        inferenceConfig={\"temperature\": 0.2, \"maxTokens\": 500}\n",
        "    )\n",
        "\n",
        "    # Extract the model's response using the correct structure\n",
        "    follow_up_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
        "    display_response(follow_up_response, \"Claude 3.7 Sonnet (Multi-turn conversation)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "id": "jMuNQBuY8L9H",
        "outputId": "0689ad49-cc2c-4462-962d-34b47b4ff47f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Response from Claude 3.7 Sonnet (Multi-turn conversation)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Amazon Bedrock is AWS's new serverless service that provides API access to foundation models from multiple AI companies, allowing customers to easily find, customize, and deploy generative AI models without managing infrastructure."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Streaming Responses with ConverseStream API"
      ],
      "metadata": {
        "id": "z2Tf5Hz5_Nv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of streaming with Converse API\n",
        "def stream_converse(model_id, messages, inference_config=None):\n",
        "    if inference_config is None:\n",
        "        inference_config = {}\n",
        "\n",
        "    print(\"Streaming response (chunks will appear as they are received):\\n\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    full_response = \"\"\n",
        "\n",
        "    try:\n",
        "        response = bedrock.converse_stream(\n",
        "            modelId=model_id,\n",
        "            messages=messages,\n",
        "            inferenceConfig=inference_config\n",
        "        )\n",
        "        response_stream = response.get('stream')\n",
        "        if response_stream:\n",
        "            for event in response_stream:\n",
        "\n",
        "                if 'messageStart' in event:\n",
        "                    print(f\"\\nRole: {event['messageStart']['role']}\")\n",
        "\n",
        "                if 'contentBlockDelta' in event:\n",
        "                    print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
        "\n",
        "                if 'messageStop' in event:\n",
        "                    print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
        "\n",
        "                if 'metadata' in event:\n",
        "                    metadata = event['metadata']\n",
        "                    if 'usage' in metadata:\n",
        "                        print(\"\\nToken usage\")\n",
        "                        print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
        "                        print(\n",
        "                            f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
        "                        print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
        "                    if 'metrics' in event['metadata']:\n",
        "                        print(\n",
        "                            f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
        "\n",
        "\n",
        "            print(\"\\n\" + \"-\" * 80)\n",
        "        return full_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in streaming: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Let's try streaming a longer summary\n",
        "streaming_request = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"text\": f\"\"\"Please provide a detailed summary of the following text, explaining its key points and implications:\n",
        "\n",
        "                {text_to_summarize}\n",
        "\n",
        "                Make your summary comprehensive but clear.\n",
        "                \"\"\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "1J0mDuN7_PNx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run this when you're ready to see streaming output\n",
        "streamed_response = stream_converse(\n",
        "    MODELS[\"Claude 3.7 Sonnet\"],\n",
        "    streaming_request,\n",
        "    inference_config={\"temperature\": 0.4, \"maxTokens\": 1000}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deiUMguh_Ujh",
        "outputId": "5f6648cc-9719-43a4-b601-ccaad82199df"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming response (chunks will appear as they are received):\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Role: assistant\n",
            "# Summary: Amazon Bedrock - Democratizing Access to Foundation Models\n",
            "\n",
            "## Key Points\n",
            "\n",
            "1. **Amazon Bedrock Introduction**: AWS has launched Amazon Bedrock, a new service providing API access to Foundation Models (FMs) from multiple providers including AI21 Labs, Anthropic, Stability AI, and Amazon itself.\n",
            "\n",
            "2. **Purpose and Value Proposition**: Bedrock aims to democratize access to generative AI by offering the easiest way for customers to build and scale applications using foundation models, regardless of technical expertise.\n",
            "\n",
            "3. **Model Offerings**: The service provides access to various powerful foundation models for both text and images, including Amazon's new Titan large language models (LLMs).\n",
            "\n",
            "4. **Service Characteristics**:\n",
            "   - Serverless experience\n",
            "   - Scalable, reliable, and secure AWS managed service\n",
            "   - No infrastructure management required\n",
            "\n",
            "5. **Key Functionalities**:\n",
            "   - Model discovery and selection based on use case\n",
            "   - Quick startup capabilities\n",
            "   - Private customization of foundation models using customer data\n",
            "   - Easy integration with existing AWS tools and services\n",
            "\n",
            "6. **Integration with AWS Ecosystem**: Bedrock works with other AWS machine learning tools like SageMaker Experiments (for model testing) and Pipelines (for managing foundation models at scale).\n",
            "\n",
            "## Implications\n",
            "\n",
            "1. **Lowered Barriers to AI Adoption**: By simplifying access to advanced AI models, AWS is enabling more businesses to leverage generative AI without needing specialized expertise.\n",
            "\n",
            "2. **Multi-vendor Approach**: AWS is positioning itself as an AI platform provider rather than just competing with its own models, offering customers choice among leading AI providers.\n",
            "\n",
            "3. **Enterprise Focus**: The emphasis on security, scalability, and private customization suggests AWS is targeting enterprise customers concerned about data privacy and integration with existing systems.\n",
            "\n",
            "4. **AI Democratization**: The service represents a significant step in making sophisticated AI capabilities accessible to a broader range of developers and organizations.\n",
            "\n",
            "5. **Competitive Response**: This appears to be AWS's strategic response to the growing demand for easy access to large language models and other foundation models in the market.\n",
            "Stop reason: end_turn\n",
            "\n",
            "Token usage\n",
            "Input tokens: 273\n",
            ":Output tokens: 465\n",
            ":Total tokens: 738\n",
            "Latency: 8585 milliseconds\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Code Generation with FMs"
      ],
      "metadata": {
        "id": "Wx1p5pSEDqK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_generation_prompt = \"\"\"\n",
        "Create a Python function called get_weather that accepts a location as parameter. \\\n",
        "The function should return a dictionary containing weather data (condition, temperature, and humidity) for predefined cities.\\\n",
        "Use a mock data structure instead of actual API calls. Include New York, San Francisco, Miami, and Seattle as default cities.\\\n",
        "The return statement should look like the following: return weather_data.get(location, {\"condition\": \"Unknown\", \"temperature\": 0, \"humidity\": 0}).\n",
        "Only return the function and no preamble or examples.\n",
        "\"\"\"\n",
        "\n",
        "converse_request = {\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"text\": f\"{code_generation_prompt}\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    \"inferenceConfig\": {\n",
        "        \"temperature\": 0.0,\n",
        "        \"topP\": 0.9,\n",
        "        \"maxTokens\": 500\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = bedrock.converse(\n",
        "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
        "        messages=converse_request[\"messages\"],\n",
        "        inferenceConfig=converse_request[\"inferenceConfig\"]\n",
        "    )\n",
        "\n",
        "    # Extract the model's response\n",
        "    claude_converse_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
        "    display_response(claude_converse_response, \"Claude 3.7 Sonnet (Converse API)\")\n",
        "except botocore.exceptions.ClientError as error:\n",
        "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
        "        print(f\"\\x1b[41m{error.response['Error']['Code']}: {error.response['Error']['Message']}\\x1b[0m\")\n",
        "        print(\"Please ensure you have the necessary permissions for Amazon Bedrock.\")\n",
        "    else:\n",
        "        raise error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "z5OSO5WlDrfS",
        "outputId": "6f0e4e71-58dc-4cde-ba4e-60829eb902e7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Response from Claude 3.7 Sonnet (Converse API)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef get_weather(location):\n    weather_data = {\n        \"New York\": {\"condition\": \"Cloudy\", \"temperature\": 72, \"humidity\": 65},\n        \"San Francisco\": {\"condition\": \"Foggy\", \"temperature\": 62, \"humidity\": 80},\n        \"Miami\": {\"condition\": \"Sunny\", \"temperature\": 85, \"humidity\": 75},\n        \"Seattle\": {\"condition\": \"Rainy\", \"temperature\": 58, \"humidity\": 90}\n    }\n    return weather_data.get(location, {\"condition\": \"Unknown\", \"temperature\": 0, \"humidity\": 0})\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weather(location):\n",
        "    weather_data = {\n",
        "        \"New York\": {\"condition\": \"Cloudy\", \"temperature\": 72, \"humidity\": 65},\n",
        "        \"San Francisco\": {\"condition\": \"Foggy\", \"temperature\": 62, \"humidity\": 80},\n",
        "        \"Miami\": {\"condition\": \"Sunny\", \"temperature\": 85, \"humidity\": 75},\n",
        "        \"Seattle\": {\"condition\": \"Rainy\", \"temperature\": 58, \"humidity\": 90}\n",
        "    }\n",
        "    return weather_data.get(location, {\"condition\": \"Unknown\", \"temperature\": 0, \"humidity\": 0})\n",
        "\n",
        "get_weather(\"New York\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mmw751UEPv0",
        "outputId": "a38507cf-58b7-4feb-f02f-a7574f87bf95"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'condition': 'Cloudy', 'temperature': 72, 'humidity': 65}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Function Calling with Amazon Bedrock\n",
        "\n",
        "Modern LLMs like Claude go beyond generating free-form text ‚Äî they can also reason about when external tools or functions should be used to better answer user questions. This capability, known as function calling (or tool use), enables the model to decide which function to call, when to call it, and what parameters to provide ‚Äî but importantly, the model does not execute the function itself.\n",
        "\n",
        "Instead, the model returns a well-structured response (typically in JSON format) that describes the intended function call. It‚Äôs then up to your application to detect this output, execute the requested function (such as calling an API or querying a database), and pass the result back to the model ‚Äî allowing it to generate a final, user-friendly response that incorporates real-world data.\n",
        "\n",
        "Function calling is especially useful when building LLM-powered applications that need access to dynamic, external information ‚Äî for example, retrieving real-time weather data, which is exactly what we‚Äôll demonstrate in the this section."
      ],
      "metadata": {
        "id": "T6OVtYPZEW5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Function Calling Flow\n",
        "Amazon Bedrock natively supports function calling through its Converse API, which provides a consistent way to give LLMs access to tools or functions that live outside the model.\n",
        "\n",
        "The typical function calling flow looks like this:\n",
        "\n",
        "- **Step 1 ‚Äî Function Recognition**. When the model (e.g., Claude 3.7) identifies that a tool should be used to answer a user query (e.g., \"What's the weather in Seattle?\"), it returns a structured response indicating:\n",
        "  - The function name to call (e.g., get_weather)\n",
        "  - The required input parameters (e.g., location=\"Seattle\")\n",
        "\n",
        "- **Step 2 ‚Äî Function Execution** (by your application). The application is responsible for:\n",
        "  - Executing the requested function\n",
        "  - Capturing the output (e.g., current weather data)\n",
        "  - Passing the result back to the model\n",
        "\n",
        "- **Step 3 ‚Äî Final Response Generation.** The LLM uses the function's output to generate a natural language response for the user."
      ],
      "metadata": {
        "id": "ldwcSqaSFvw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Example: Implementation of Weather Function Calling with Bedrock's Converse API\n",
        "\n",
        "we‚Äôll integrate a simple get_weather function with Claude 3.7 using the Converse API. The tool specification we provide to Claude defines:\n",
        "\n",
        "- The function name: get_weather\n",
        "- The function's purpose: \"Retrieve weather for a given location\"\n",
        "- The required input parameters: location (string)\n",
        "\n",
        "Example Workflow:\n",
        "- User: \"What's the weather in Seattle?\"\n",
        "‚Üì\n",
        "- LLM: Returns function call request for `get_weather(\"Seattle\")`\n",
        "‚Üì\n",
        "- Application: Executes weather lookup, and feeds the result back to the LLM\n",
        "‚Üì\n",
        "- LLM: Generates response using actual weather data"
      ],
      "metadata": {
        "id": "vSRFZU98UF7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weather(location):\n",
        "    weather_data = {\n",
        "        \"New York\": {\"condition\": \"Cloudy\", \"temperature\": 72, \"humidity\": 65},\n",
        "        \"San Francisco\": {\"condition\": \"Foggy\", \"temperature\": 62, \"humidity\": 80},\n",
        "        \"Miami\": {\"condition\": \"Sunny\", \"temperature\": 85, \"humidity\": 75},\n",
        "        \"Seattle\": {\"condition\": \"Rainy\", \"temperature\": 58, \"humidity\": 90}\n",
        "    }\n",
        "    return weather_data.get(location, {\"condition\": \"Unknown\", \"temperature\": 0, \"humidity\": 0})\n",
        "\n",
        "\n",
        "weather_tool = {\n",
        "    \"tools\": [\n",
        "        {\n",
        "            \"toolSpec\": {\n",
        "                \"name\": \"get_weather\",\n",
        "                \"description\": \"Get current weather for a specific location\",\n",
        "                \"inputSchema\": {\n",
        "                    \"json\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"location\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"The city name to get weather for\"\n",
        "                            }\n",
        "                        },\n",
        "                        \"required\": [\"location\"]\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    ],\n",
        "    \"toolChoice\": {\n",
        "        \"auto\": {}  # Let the model decide when to use the tool\n",
        "    }\n",
        "}\n",
        "\n",
        "function_request = {\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"text\": \"What's the weather like in San Francisco right now? And what should I wear?\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    \"inferenceConfig\": {\n",
        "        \"temperature\": 0.0,  # Use 0 temperature for deterministic function calling\n",
        "        \"maxTokens\": 500\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "QZTfkrTEUZxy"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the tool specification and the function_request we can now invoke the model and take a look it it's response. Observe how it correctly fills out the \"toolUse\" parameter in its response."
      ],
      "metadata": {
        "id": "4yXlgc1rUpEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = bedrock.converse(\n",
        "    modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
        "    messages=function_request[\"messages\"],\n",
        "    inferenceConfig=function_request[\"inferenceConfig\"],\n",
        "    toolConfig=weather_tool\n",
        ")\n",
        "print(json.dumps(response, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUckc2hVUgZL",
        "outputId": "bc923f2f-a534-41b3-da38-a69f26e49046"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"ResponseMetadata\": {\n",
            "    \"RequestId\": \"499a1f32-06dd-4b03-b001-1ca72c3392aa\",\n",
            "    \"HTTPStatusCode\": 200,\n",
            "    \"HTTPHeaders\": {\n",
            "      \"date\": \"Tue, 02 Dec 2025 13:48:51 GMT\",\n",
            "      \"content-type\": \"application/json\",\n",
            "      \"content-length\": \"570\",\n",
            "      \"connection\": \"keep-alive\",\n",
            "      \"x-amzn-requestid\": \"499a1f32-06dd-4b03-b001-1ca72c3392aa\"\n",
            "    },\n",
            "    \"RetryAttempts\": 0\n",
            "  },\n",
            "  \"output\": {\n",
            "    \"message\": {\n",
            "      \"role\": \"assistant\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"text\": \"I can check the current weather in San Francisco for you and provide clothing recommendations based on that information.\"\n",
            "        },\n",
            "        {\n",
            "          \"toolUse\": {\n",
            "            \"toolUseId\": \"tooluse_8f3RHsKNTuGNvHkKcp8_2g\",\n",
            "            \"name\": \"get_weather\",\n",
            "            \"input\": {\n",
            "              \"location\": \"San Francisco\"\n",
            "            },\n",
            "            \"type\": \"tool_use\"\n",
            "          }\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"stopReason\": \"tool_use\",\n",
            "  \"usage\": {\n",
            "    \"inputTokens\": 404,\n",
            "    \"outputTokens\": 75,\n",
            "    \"totalTokens\": 479,\n",
            "    \"cacheReadInputTokens\": 0,\n",
            "    \"cacheWriteInputTokens\": 0\n",
            "  },\n",
            "  \"metrics\": {\n",
            "    \"latencyMs\": 1561\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_function_calling(model_id, request, tool_config):\n",
        "    try:\n",
        "        # Step 1: Send initial request\n",
        "        response = bedrock.converse(\n",
        "            modelId=model_id,\n",
        "            messages=request[\"messages\"],\n",
        "            inferenceConfig=request[\"inferenceConfig\"],\n",
        "            toolConfig=tool_config\n",
        "        )\n",
        "\n",
        "        # Check if the model wants to use a tool (check the correct response structure)\n",
        "        content_blocks = response[\"output\"][\"message\"][\"content\"]\n",
        "        has_tool_use = any(\"toolUse\" in block for block in content_blocks)\n",
        "\n",
        "        if has_tool_use:\n",
        "            # Find the toolUse block\n",
        "            tool_use_block = next(block for block in content_blocks if \"toolUse\" in block)\n",
        "            tool_use = tool_use_block[\"toolUse\"]\n",
        "            tool_name = tool_use[\"name\"]\n",
        "            tool_input = tool_use[\"input\"]\n",
        "            tool_use_id = tool_use[\"toolUseId\"]\n",
        "\n",
        "            # Step 2: Execute the tool\n",
        "            if tool_name == \"get_weather\":\n",
        "                tool_result = get_weather(tool_input[\"location\"])\n",
        "            else:\n",
        "                tool_result = {\"error\": f\"Unknown tool: {tool_name}\"}\n",
        "\n",
        "            # Step 3: Send the tool result back to the model\n",
        "            updated_messages = request[\"messages\"] + [\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"toolUse\": {\n",
        "                                \"toolUseId\": tool_use_id,\n",
        "                                \"name\": tool_name,\n",
        "                                \"input\": tool_input\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"toolResult\": {\n",
        "                                \"toolUseId\": tool_use_id,\n",
        "                                \"content\": [\n",
        "                                    {\n",
        "                                        \"json\": tool_result\n",
        "                                    }\n",
        "                                ],\n",
        "                                \"status\": \"success\"\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            # Step 4: Get final response\n",
        "            final_response = bedrock.converse(\n",
        "                modelId=model_id,\n",
        "                messages=updated_messages,\n",
        "                inferenceConfig=request[\"inferenceConfig\"],\n",
        "                toolConfig=tool_config\n",
        "            )\n",
        "\n",
        "            # Extract text from the correct response structure\n",
        "            final_text = \"\"\n",
        "            for block in final_response[\"output\"][\"message\"][\"content\"]:\n",
        "                if \"text\" in block:\n",
        "                    final_text = block[\"text\"]\n",
        "                    break\n",
        "\n",
        "            return {\n",
        "                \"tool_call\": {\"name\": tool_name, \"input\": tool_input},\n",
        "                \"tool_result\": tool_result,\n",
        "                \"final_response\": final_text\n",
        "            }\n",
        "        else:\n",
        "            # Model didn't use a tool, just return the text response\n",
        "            text_response = \"\"\n",
        "            for block in content_blocks:\n",
        "                if \"text\" in block:\n",
        "                    text_response = block[\"text\"]\n",
        "                    break\n",
        "\n",
        "            return {\n",
        "                \"final_response\": text_response\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in function calling: {str(e)}\")\n",
        "        return {\"error\": str(e)}"
      ],
      "metadata": {
        "id": "k4TJlvBtUxUK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function_result = handle_function_calling(\n",
        "    MODELS[\"Claude 3.7 Sonnet\"],\n",
        "    function_request,\n",
        "    weather_tool\n",
        ")\n",
        "\n",
        "# Display the results\n",
        "if \"error\" not in function_result:\n",
        "    if \"tool_call\" in function_result:\n",
        "        print(f\"Tool Call: {function_result['tool_call']['name']}({function_result['tool_call']['input']})\")\n",
        "        print(f\"Tool Result: {function_result['tool_result']}\")\n",
        "\n",
        "    display_response(function_result[\"final_response\"], \"Claude 3.7 Sonnet (Function Calling)\")\n",
        "else:\n",
        "    print(f\"Error: {function_result['error']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "VGBz5pRsU24n",
        "outputId": "50434ab6-6a6e-4f04-f9ab-68fac0226970"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool Call: get_weather({'location': 'San Francisco'})\n",
            "Tool Result: {'condition': 'Foggy', 'temperature': 62, 'humidity': 80}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Response from Claude 3.7 Sonnet (Function Calling)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Currently in San Francisco, it's 62¬∞F with foggy conditions and 80% humidity.\n\nFor clothing recommendations based on this weather:\n- A light jacket or sweater would be appropriate for the mild temperature\n- Consider layering since San Francisco weather can change throughout the day\n- The fog might make it feel a bit cooler than the temperature suggests\n- Comfortable walking shoes are always good for San Francisco's hills\n- You might want to bring a hat or light scarf if you're sensitive to the damp foggy air\n\nSan Francisco is known for its microclimates, so if you're planning to visit different neighborhoods, having an extra layer available is always a good idea."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}