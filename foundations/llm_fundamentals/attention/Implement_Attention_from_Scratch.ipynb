{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Attention from Scratch"
      ],
      "metadata": {
        "id": "ZyWvw0w9LVVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement"
      ],
      "metadata": {
        "id": "JHQqNeB1LSeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a **Scaled Dot-Product Attention** mechanism from scratch using PyTorch. Mission is to replicate what PyTorch's built-in `scaled_dot_product_attention` does â€” manually.\n",
        "\n",
        "This core component is essential in Transformer architectures and helps models focus on relevant parts of a sequence. You'll test your implementation against PyTorch's native one to ensure you nailed it.\n",
        "\n",
        "### Requirements\n",
        "1. Define the Function:\n",
        "   - Create a function `scaled_dot_product_attention(q, k, v, mask=None)` that:\n",
        "     - Computes attention scores via the dot product of query and key vectors.\n",
        "     - Scales the scores using the square root of the key dimension.\n",
        "     - Applies an optional mask to the scores.\n",
        "     - Applies softmax to convert scores into attention weights.\n",
        "     - Uses these weights to compute a weighted sum of values (V).\n",
        "2. Test Your Work:\n",
        "   - Use sample tensors for query (Q), key (K), and value (V).\n",
        "   - Compare the result of your custom implementation with PyTorch's `F.scaled_dot_product_attention` using an assert to check numerical accuracy.\n",
        "\n",
        "### Constraints\n",
        "- âŒ Do NOT use F.scaled_dot_product_attention inside your custom function â€” that defeats the whole point.\n",
        "- âœ… Your implementation must handle batch dimensions correctly.\n",
        "- âœ… Support optional masking for future tokens or padding.\n",
        "- âœ… Use only PyTorch ops â€” no cheating with external attention libs.\n",
        "\n",
        "ðŸ’¡ Hint:\n",
        "- Use `torch.matmul()` to compute dot products and `F.softmax()` for the final attention weights.\n",
        "- The mask (if used) should be applied **before** the softmax using `masked_fill`\n"
      ],
      "metadata": {
        "id": "wEFikAuILZvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rephrase"
      ],
      "metadata": {
        "id": "p91SIQIbWhII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function `scaled_dot_product_attention(q, k, v, mask=None)` that manually replicates PyTorch's built-in attention.\n",
        "\n",
        "- **Compute attention scores**: Dot product between queries and keys using `torch.matmul()`\n",
        "- **Scale scores**: Divide by square root of key dimension (`sqrt(d_k)`)\n",
        "- **Apply mask** (optional): Use `masked_fill()` for future tokens or padding\n",
        "- **Compute attention weights**: Apply `F.softmax()` to scaled scores.\n",
        "- **Compute output**: Weighted sum of values using attention weights\n",
        "\n",
        "**Validation**: Compare your implementation with `F.scaled_dot_product_attention()` using numerical assertion.\n",
        "\n",
        "**Constraints**:\n",
        "- Use only basic PyTorch operations (no high-level attention functions)\n",
        "- Handle batch dimensions correctly\n",
        "- Support optional masking"
      ],
      "metadata": {
        "id": "Yg1DiNoKWicg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why is it important - understand how the Transformer \"brain\" works.\n",
        "\n",
        "The goal is to replicate the logic of F.scaled_dot_product_attention, breaking it down into key steps:\n",
        "\n",
        "- **Dot-product:** `Q * Káµ€` â†’ \"How similar is each query to each key?\"\n",
        "- **Scaling:** `âˆšdâ‚–` â†’ prevents softmax from exploding at large `dâ‚–` (due to reduced variance of dot-products),\n",
        "- **Masking (optional)**: `masked_fill(-inf)` â†’ blocks attention to forbidden positions (padding, future tokens),\n",
        "- **Softmax**: â†’ converts scores into probabilities (weights),\n",
        "Weighted sum: weights V â†’ aggregates information from values â€‹â€‹based on relevance.\n",
        "\n",
        "Important:\n",
        "- Support for batch dimensions `(Batch_size, Num_Heads, Sequence_Length, Dimension_per_head)`,\n",
        "- Numerical equivalence with F.scaled_dot_product_attention (checked via `torch.allclose(..., atol=1e-6)),`\n",
        "- No internal calls to `F.scaled_dot_product_attention` â€” only basic torch operations."
      ],
      "metadata": {
        "id": "HlJ0UiCcZUw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretic"
      ],
      "metadata": {
        "id": "UbfwsJYSj-Cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention is a mechanism that lets neural networks focus on specific parts of an input sequence.\n",
        "\n",
        "A fundamental type is Scaled Dot-Product Attention (used in Transformer). It has three inputs:\n",
        "\n",
        "- Query (Q): The current token trying to gather information.\n",
        "- Key (K): A representation of each token in the sequence thatâ€™s available to be attended to.\n",
        "- Value (V): What each token provides if selected by the attention mechanism\n",
        "\n",
        "### Scaled Dot-Product Attention Concept\n",
        "\n",
        "| Component | Mathematics | Analogy | Dimensionality | Why is it needed? |\n",
        "|-----------|------------|----------|-------------|--------------|\n",
        "| **Query (Q)** | Query vector | \"What am I looking for?\" | `[batch, seq_len, d_k]` | Represents the interest of the current position |\n",
        "| **Key (K)** | Key vector | \"What can I offer?\" | `[batch, seq_len, d_k]` | Characterizes how relevant a token is to others |\n",
        "| **Value (V)** | Value vector | \"What information do I carry?\" | `[batch, seq_len, d_v]` | Contains the actual information for aggregation |\n",
        "| **`QÂ·Káµ€`** | `matmul(Q, K.transpose(-2,-1))` | \"How well do the questions and answers match?\" | `[batch, seq_len, seq_len]` | Calculates the pairwise similarity of all tokens |\n",
        "| **Scaling** | `Ã· âˆšd_k` | \"Normalization of estimates\" | - | Stabilizes gradients at high dimensions |\n",
        "| **Masking** | `masked_fill(mask, -1e9)` | \"Ignore forbidden positions\" | `[batch, seq_len, seq_len]` | Prevents attention to padding/future tokens |\n",
        "| **Softmax** | `exp(x) / âˆ‘exp(x)` | \"Convert to probabilities\" | `[batch, seq_len, seq_len]` | Converts scores to weights (sum=1) |\n",
        "| **Output** | `weights @ V` | \"Weighted summation of information\" | `[batch, seq_len, d_v]` | Aggregates values â€‹â€‹by relevance |\n",
        "\n",
        "### Scaled Dot-Product Attention calculation step-by-step:\n",
        "1. We measure how relevant each key `K` is to our query `Q` using a dot product: `QÂ·Káµ€`.\n",
        "2. To keep the values stable for large embeddings, we divide by `âˆšd_k`, where `d_k` is the dimensionality of the key vectors: `Scaling = QÂ·Káµ€ / âˆšd_k`.\n",
        " When the `d_k` is large, the dot product can grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. For dot products, the variance grows with `d_k`. That's why the square root is used instead - we normalize the scale.\n",
        "3. Convert the scores into a probability distribution to see how much attention should be given to each element: `Softmax = softmax(Scaling)`.\n",
        "4. Multiply each value `V` by its attention `weight` and sum to get the final output: `Attention(Q, K, V) =  Softmax * V`.\n",
        "This yields a context vector that highlights the most relevant information from `V` for the query `Q`.\n",
        "\n",
        "**In short**: attention computes a weighted sum of input elements (values) where the weights are determined by a compatibility function between a query and corresponding keys: `Attention(Q, K, V) =  softmax(QÂ·Káµ€/âˆšd_k)Â·V`\n",
        "\n",
        "**Simplifications:**\n",
        "\n",
        "> Imagine you're at a large party trying to focus on a specific conversation. You're asking yourself about each person: \"How relevant is what this person is saying to what I want to know?\" (computing attention scores). Then you focus more on people providing useful information (applying the attention weights) while still maintaining some awareness of everyone else. Your brain combines all this information, giving more weight to important sources (weighted sum of values).\n",
        "\n",
        "Or\n",
        "\n",
        "> A simple explanation: attention is just a dictionary with approximation. In a usual dictionary we have a pair of key-value and we pass a query to get a result. We either get the value of the key or nothing. In attention we get the answer even if we can't find the exact key.\n",
        "\n",
        "## The essence of the dimensions d_k and d_v\n",
        "`d_k` (**dimension of keys/queries**) and `d_v` (**dimension of values**) are not fixed values, but rather calculated parameters that are determined by the architecture of the attention mechanism and the optimization tasks.\n",
        "\n",
        "- Self-Attention: Q, K, V from one source â†’ usually `d_k = d_v`\n",
        "- Cross-Attention: Q from one source, K, V from another â†’ `d_k` and `d_v` can differ\n",
        "- Multi-Head Attention: embedding is divided into heads â†’ `d_k = d_v = embedding_dim / num_heads`\n",
        "- MultiQuery Attention: shared K, V for all heads â†’ memory savings\n",
        "\n",
        "| Attention | Essence | Analogy | Pattern | Using | Memory |\n",
        "|-----------|---------|---------|---------------------|------|-----|\n",
        "|**Self-Attention**|Tokens of the same sequence look at each other|Group discussion | `Q,K,V = f(same_sequence)`|Text Comprehension (BERT)| Average\n",
        "|**Cross-Attention** | Queries from sequence A, Keys/Values from sequence B | Student asks textbook | `Q = f(A)`, `K,V = f(B)` |Machine translation, chatbots|Average|\n",
        "| **Multi-Head** | Many \"experts\" for different aspects | Team of experts | `split â†’ Attention_i â†’ concat` | Transformers (GPT, BERT)|High|\n",
        "| **MultiQuery** | Shared K,V for all heads to save memory | One reference book for whole class | `K,V` shared across heads |Fast inference models (Llama 2)|Low|\n",
        "\n",
        "More about types of Attentions check in the application (at the end of the current file)\n",
        "\n",
        "### Multi-Head Attention\n",
        "`d_k = d_v = embedding_dim // num_heads`, where:\n",
        "- `embedding_dim` â€” total embedding dimension (512, 768, 1024)\n",
        "- `num_heads` â€” number of attention heads (8, 12, 16)\n",
        "\n",
        "\n",
        "### Optimization tasks\n",
        "- Model quality: large `d_k/d_v` â†’ higher capacity\n",
        "- Efficiency: small `d_k/d_v` â†’ faster computation\n",
        "- Memory: MultiQuery â†’ less memory for `K`, `V`"
      ],
      "metadata": {
        "id": "mvpUL9BZkAai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Solution\n"
      ],
      "metadata": {
        "id": "mMguqkhq_elL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple version without `causal_mask`"
      ],
      "metadata": {
        "id": "acUUKL4lPUqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "  d_k = k.size(-1)\n",
        "\n",
        "  # Step 1: Calculate Similarity\n",
        "  scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "  # Step 2: Scaling\n",
        "  scores = scores / math.sqrt(d_k) # in Pytorch: scores / math.sqrt(d_k)\n",
        "\n",
        "  # Step 3: Masking (if provided)\n",
        "  if mask is not None:\n",
        "    # mask: True = block\n",
        "    scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "  # Step 4: Softmax -> attention weights\n",
        "  attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "  # Step 5: Weighted sum of values\n",
        "  output = torch.matmul(attn_weights, v)\n",
        "\n",
        "  return output\n",
        "\n",
        "# def create_causal_mask(seq_len, batch_size=1):\n",
        "#   return torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()"
      ],
      "metadata": {
        "id": "PnDpd-3K_heL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimal, correct test:\n",
        "\n",
        "- identical inputs\n",
        "- `dropout_p=0.0`\n",
        "- `is_causal=False`\n",
        "- `eval()` mode is not needed (dropout is disabled)"
      ],
      "metadata": {
        "id": "-5_ZgHVcMfJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "# =========================\n",
        "# TEST\n",
        "# =========================\n",
        "torch.manual_seed(42)\n",
        "\n",
        "B = 2   # batch size\n",
        "L = 5   # sequence length\n",
        "D = 8   # embedding dim\n",
        "\n",
        "q = torch.randn(B, L, D)\n",
        "k = torch.randn(B, L, D)\n",
        "v = torch.randn(B, L, D)\n",
        "\n",
        "out_custom = scaled_dot_product_attention(q, k, v)\n",
        "\n",
        "out_torch = F.scaled_dot_product_attention(\n",
        "    q, k, v,\n",
        "    attn_mask=None,\n",
        "    dropout_p=0.0,\n",
        "    is_causal=False\n",
        ")\n",
        "\n",
        "# Check figures\n",
        "torch.testing.assert_close(\n",
        "    out_custom,\n",
        "    out_torch,\n",
        "    rtol=1e-5,\n",
        "    atol=1e-6\n",
        ")\n",
        "\n",
        "print(\"âœ… Test passed: custom implementation matches PyTorch.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNbKJHWaMstM",
        "outputId": "0311947b-741c-4b40-c1aa-e2ab12513751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Test passed: custom implementation matches PyTorch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version with Casual mask\n",
        "Removing future connections from a probability distribution"
      ],
      "metadata": {
        "id": "o_SILGbDPc4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is masking applied before softmax:\n",
        "- `exp(-inf) = 0`\n",
        "- the weights of future tokens become `0`\n",
        "- row sum = `1`"
      ],
      "metadata": {
        "id": "BzTxpxpiOxpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ð¡ausal mask**\n",
        "At position `t` the model has no right to look into the future `(t+1, t+2, ...)`:\n",
        "```css\n",
        "Posision 0 -> [0]\n",
        "Posision 1 -> [0,1]\n",
        "Posision 2 -> [0,1,2]\n",
        "Posision 3 -> [0,1,2,3]\n",
        "Posision 4 -> [0,1,2,3,4]\n",
        "```\n",
        "\n",
        "All elements to the right of the diagonal must be masked:\n",
        "- `1` - Disable attention and\n",
        "- `0` - Allow\n",
        "```css\n",
        "[[0, 1, 1, 1, 1],\n",
        " [0, 0, 1, 1, 1],\n",
        " [0, 0, 0, 1, 1],\n",
        " [0, 0, 0, 0, 1],\n",
        " [0, 0, 0, 0, 0]]\n",
        "```\n",
        "\n",
        "[torch.triu](https://docs.pytorch.org/docs/stable/generated/torch.triu.html):\n",
        "- `torch.triu(..., diagonal=1)` - everything above the main diagonal\n",
        "- `bool` - masked_fill awaiting"
      ],
      "metadata": {
        "id": "G6qaqjzVNGyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L = q.size(-2)\n",
        "\n",
        "causal_mask = torch.triu(\n",
        "    torch.ones(L, L, dtype=torch.bool),\n",
        "    diagonal=1\n",
        ")\n",
        "causal_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzrpYrX5N-HV",
        "outputId": "8eb67d06-5406-4ee9-a9c2-68032a812987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True,  True,  True,  True],\n",
              "        [False, False,  True,  True,  True],\n",
              "        [False, False, False,  True,  True],\n",
              "        [False, False, False, False,  True],\n",
              "        [False, False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def scaled_dot_product_attention_custom(\n",
        "  q, k, v,\n",
        "  attn_mask=None,\n",
        "  is_causal=False\n",
        "):\n",
        "\n",
        "  d_k = k.size(-1)\n",
        "\n",
        "  # Step 1: Calculate Similarity\n",
        "  scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "  # Step 2: Scaling\n",
        "  scores = scores / math.sqrt(d_k) # in Pytorch: scores / math.sqrt(d_k)\n",
        "\n",
        "  # Step 3a: Masking (if provided)\n",
        "  if is_causal:\n",
        "    L = q.size(-2)\n",
        "    causal_mask = torch.triu(\n",
        "      torch.ones(L, L, device=q.device, dtype=torch.bool),\n",
        "      diagonal=1 # True above the diagonal; Diagonal itself is NOT touched\n",
        "    )\n",
        "    # If causal_mask[t, j] == True: scores[t, j] = -âˆž -> softmax(-âˆž) = 0\n",
        "    scores = scores.masked_fill(causal_mask, float('-inf'))\n",
        "\n",
        "  # Step 3b: attention mask (padding / custom)\n",
        "  if attn_mask is not None:\n",
        "    scores = scores.masked_fill(attn_mask, float('-inf'))\n",
        "\n",
        "  # Step 4: Softmax -> attention weights\n",
        "  attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "  # Step 5: Weighted sum of values\n",
        "  output = torch.matmul(attn_weights, v)\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "GPfHPQ2IP5Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test: Custom mask - Comparison with PyTorch (causal)"
      ],
      "metadata": {
        "id": "Ua2pvEZDNAmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "B, L, D = 2, 6, 8\n",
        "\n",
        "q = torch.randn(B, L, D)\n",
        "k = torch.randn(B, L, D)\n",
        "v = torch.randn(B, L, D)\n",
        "\n",
        "out_custom = scaled_dot_product_attention_custom(\n",
        "    q, k, v,\n",
        "    is_causal=True\n",
        ")\n",
        "\n",
        "out_torch = F.scaled_dot_product_attention(\n",
        "    q, k, v,\n",
        "    is_causal=True,\n",
        "    dropout_p=0.0\n",
        ")\n",
        "\n",
        "torch.testing.assert_close(\n",
        "    out_custom,\n",
        "    out_torch,\n",
        "    rtol=1e-5,\n",
        "    atol=1e-6\n",
        ")\n",
        "\n",
        "print(\"âœ… Causal attention test passed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe3ces_xQTf9",
        "outputId": "54f0e54a-8f91-4cae-a665-6b22d91bf7a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Causal attention test passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Padding mask - Comparison with PyTorch (causal)\n",
        "\n",
        "Since I am implementing logic that is fully compatible with PyTorch, it is important to note PyTorch internal logic:\n",
        "1. `is_causal=True`\n",
        "   - PyTorch **generates the causal mask itself**\n",
        "   - `attn_mask` must be `None`\n",
        "   - Passing both results in undefined behavior\n",
        "2. `attn_mask`\n",
        "   - Either bool mask (`True = disable`)\n",
        "   - or additive mask (`0 / -inf`)\n",
        "   - used for **padding/arbitrary** masking\n",
        "\n",
        "In `F.scaled_dot_product_attention` is a hard contract:\n",
        "```is_causal=True  âŸ¹  attn_mask MUST be None```. This is due to optimizations in FlashAttention kernels that require simplified logic (for performance):\n",
        " - is_causal=True enables specialized kernels\n",
        "    - FlashAttention\n",
        "    - Memory-efficient attention\n",
        " - These kernels cannot simultaneously:\n",
        "    - causal masking\n",
        "    - arbitrary mask\n",
        "\n",
        "#### What is a padding mask (in practice)\n",
        "Needs to ignore padding tokens in attention.\n",
        "- all columns `PAD` â†’ `-inf`\n",
        "- softmax will yield zeros\n",
        "\n",
        "! The padding mask always masks keys,\n",
        "not queries."
      ],
      "metadata": {
        "id": "Vkx-rfGA1rpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1 - real token, 0 - padding\n",
        "padding_mask = torch.tensor([\n",
        "    [1, 1, 1, 0, 0],\n",
        "    [1, 1, 1, 1, 0],\n",
        "])\n",
        "# scores[b, i, j] = Q[b, i] Â· K[b, j] -> [B, L_query, L_key]\n",
        "attn_mask = padding_mask[:, None, :] == 0\n",
        "padding_mask.shape, attn_mask.shape, padding_mask, attn_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67WFns2B4ULL",
        "outputId": "535ed955-9301-49eb-9c4d-764e504d56db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 5]),\n",
              " torch.Size([2, 1, 5]),\n",
              " tensor([[1, 1, 1, 0, 0],\n",
              "         [1, 1, 1, 1, 0]]),\n",
              " tensor([[[False, False, False,  True,  True]],\n",
              " \n",
              "         [[False, False, False, False,  True]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = torch.zeros(2, 5, 5)\n",
        "scores.masked_fill(attn_mask, float('-inf'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw9J13NT6ik1",
        "outputId": "03f5e2f8-cf1a-4b59-d474-58d377710f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., -inf, -inf],\n",
              "         [0., 0., 0., -inf, -inf],\n",
              "         [0., 0., 0., -inf, -inf],\n",
              "         [0., 0., 0., -inf, -inf],\n",
              "         [0., 0., 0., -inf, -inf]],\n",
              "\n",
              "        [[0., 0., 0., 0., -inf],\n",
              "         [0., 0., 0., 0., -inf],\n",
              "         [0., 0., 0., 0., -inf],\n",
              "         [0., 0., 0., 0., -inf],\n",
              "         [0., 0., 0., 0., -inf]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "B, L, D = 2, 6, 8\n",
        "\n",
        "q = torch.randn(B, L, D)\n",
        "k = torch.randn(B, L, D)\n",
        "v = torch.randn(B, L, D)\n",
        "\n",
        "padding_mask = torch.tensor([\n",
        "    [1, 1, 1, 0, 0, 0],\n",
        "    [1, 1, 1, 1, 0, 0],\n",
        "], dtype=torch.bool)\n",
        "\n",
        "attn_mask = ~padding_mask[:, None, :]\n",
        "\n",
        "out_custom = scaled_dot_product_attention_custom(\n",
        "    q, k, v,\n",
        "    attn_mask=attn_mask,\n",
        "    is_causal=False\n",
        ")\n",
        "\n",
        "out_torch = F.scaled_dot_product_attention(\n",
        "    q, k, v,\n",
        "    attn_mask=attn_mask,\n",
        "    dropout_p=0.0,\n",
        "    is_causal=False\n",
        ")\n",
        "\n",
        "torch.testing.assert_close(\n",
        "    out_custom,\n",
        "    out_torch,\n",
        "    rtol=1e-5,\n",
        "    atol=1e-6\n",
        ")\n",
        "\n",
        "print(\"âœ… Causal attention test with Padding Mask passed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "RahqFoHI7H13",
        "outputId": "1f99cf17-e903-43ee-c2e5-dbd124e7dd3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Tensor-likes are not close!\n\nMismatched elements: 96 / 96 (100.0%)\nGreatest absolute difference: 2.1699323654174805 at index (1, 2, 4) (up to 1e-06 allowed)\nGreatest relative difference: 105.48564910888672 at index (0, 1, 4) (up to 1e-05 allowed)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2710179144.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m torch.testing.assert_close(\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mout_custom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mout_torch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/testing/_comparison.py\u001b[0m in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror_metas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0;31m# TODO: compose all metas into one AssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror_metas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 96 / 96 (100.0%)\nGreatest absolute difference: 2.1699323654174805 at index (1, 2, 4) (up to 1e-06 allowed)\nGreatest relative difference: 105.48564910888672 at index (0, 1, 4) (up to 1e-05 allowed)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow 100% mismaching!\n",
        "Probably, I had a symantic issue in comparing with Pytorch.\n",
        "\n",
        "In my realization:\n",
        "- `scores = scores.masked_fill(attn_mask, -inf)` -> `True` - DISABLED attention\n",
        "\n",
        "In Pytorch (`F.scaled_dot_product_attention`:\n",
        "- `attn_mask == True` â†’ ALLOWED\n",
        "- `attn_mask == False` â†’ DISABLED\n",
        "\n",
        "Because of that I have 100% mismatch:\n",
        " - I prohibit certain positions\n",
        " - PyTorch allows these positions\n",
        "\n",
        "\n",
        "Why PyTorch did this:\n",
        "- `attn_mask` was designed as an attention bias\n",
        "- `True = allowed`, convenient for flash-attention/kernels\n",
        "- `masked_fill` is simply a low-level API with different logic\n",
        "\n",
        "Let inverted attn_mask in the realization for accept mask in the same semantics as PyTorch."
      ],
      "metadata": {
        "id": "NawAmotY83ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_mask_test = torch.tensor([True, True, False])\n",
        "~attn_mask_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjWh1eu6_2PP",
        "outputId": "581752e3-f035-4b08-81d0-03197d61c9d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False, False,  True])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def scaled_dot_product_attention_custom(\n",
        "  q, k, v,\n",
        "  attn_mask=None,\n",
        "  is_causal=False,\n",
        "  dropout_p=0.0\n",
        "):\n",
        "\n",
        "  # API CONTRACT (PyTorch is not allow use is_causal=True and attn_mask)\n",
        "  if is_causal and attn_mask is not None:\n",
        "    raise RuntimeError(\n",
        "      \"Explicit attn_mask should not be set when is_causal=True\"\n",
        "    )\n",
        "\n",
        "  d_k = k.size(-1)\n",
        "\n",
        "  # Step 1: Calculate Similarity\n",
        "  scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "  # Step 2: Scaling\n",
        "  scores = scores / math.sqrt(d_k) # in Pytorch: scores / math.sqrt(d_k)\n",
        "\n",
        "  # Step 3a: Masking (if provided)\n",
        "  if is_causal:\n",
        "    L = q.size(-2)\n",
        "    causal_mask = torch.triu(\n",
        "      torch.ones(L, L, device=q.device, dtype=torch.bool),\n",
        "      diagonal=1 # True above the diagonal; Diagonal itself is NOT touched\n",
        "    )\n",
        "    # If causal_mask[t, j] == True: scores[t, j] = -âˆž -> softmax(-âˆž) = 0\n",
        "    scores = scores.masked_fill(causal_mask, float('-inf'))\n",
        "\n",
        "  # Step 3b: attention mask (padding / custom)\n",
        "  if attn_mask is not None:\n",
        "    scores = scores.masked_fill(~attn_mask, float('-inf'))\n",
        "\n",
        "  # Step 4: Softmax -> attention weights\n",
        "  attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "  # Step 5: Weighted sum of values\n",
        "  output = torch.matmul(attn_weights, v)\n",
        "\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "K6qMYkUb_-1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test pedding"
      ],
      "metadata": {
        "id": "tnQGRSvkCVyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "B, L, D = 2, 6, 8\n",
        "\n",
        "q = torch.randn(B, L, D)\n",
        "k = torch.randn(B, L, D)\n",
        "v = torch.randn(B, L, D)\n",
        "\n",
        "padding_mask = torch.tensor([\n",
        "    [1, 1, 1, 0, 0, 0],\n",
        "    [1, 1, 1, 1, 0, 0],\n",
        "], dtype=torch.bool)\n",
        "\n",
        "attn_mask = ~padding_mask[:, None, :]\n",
        "\n",
        "out_custom = scaled_dot_product_attention_custom(\n",
        "    q, k, v,\n",
        "    attn_mask=attn_mask,\n",
        "    is_causal=False\n",
        ")\n",
        "\n",
        "out_torch = F.scaled_dot_product_attention(\n",
        "    q, k, v,\n",
        "    attn_mask=attn_mask,\n",
        "    dropout_p=0.0,\n",
        "    is_causal=False\n",
        ")\n",
        "\n",
        "torch.testing.assert_close(\n",
        "    out_custom,\n",
        "    out_torch,\n",
        "    rtol=1e-5,\n",
        "    atol=1e-6\n",
        ")\n",
        "\n",
        "print(\"âœ… Causal attention test with Padding Mask passed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCo1GleLCZa7",
        "outputId": "8f0563c5-8f62-4970-98e0-5c2a4b652ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Causal attention test with Padding Mask passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test causal + pedding"
      ],
      "metadata": {
        "id": "LrBqkKvpCQYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assert_raises_same_error(fn1, fn2, *args, **kwargs):\n",
        "    err1 = err2 = None\n",
        "\n",
        "    try:\n",
        "        fn1(*args, **kwargs)\n",
        "    except Exception as e:\n",
        "        err1 = e\n",
        "\n",
        "    try:\n",
        "        fn2(*args, **kwargs)\n",
        "    except Exception as e:\n",
        "        err2 = e\n",
        "\n",
        "    assert err1 is not None, \"Custom function did not raise\"\n",
        "    assert err2 is not None, \"PyTorch function did not raise\"\n",
        "\n",
        "    assert type(err1) is type(err2), \\\n",
        "        f\"Error types differ: {type(err1)} vs {type(err2)}\"\n",
        "\n",
        "    assert str(err1) in str(err2)\n",
        "\n",
        "    print(\"âœ… Both implementations raise the same error\")\n",
        "\n",
        "assert_raises_same_error(\n",
        "    scaled_dot_product_attention_custom,\n",
        "    F.scaled_dot_product_attention,\n",
        "    q, k, v,\n",
        "    attn_mask=attn_mask,\n",
        "    dropout_p=0.0,\n",
        "    is_causal=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhkC2ELnCahJ",
        "outputId": "cf7bae95-6f0a-49c9-ef9b-1e0b5fc8de78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Both implementations raise the same error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropout in attention\n",
        "\n",
        "`dropout_p` is a regularization tool important for stable training and preventing overfitting, especially on large models/long sequences."
      ],
      "metadata": {
        "id": "-AjGs2w37QZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In traditional neural networks, dropout is used for regularization: parts of a neuron/activations are randomly \"turned off\" to **prevent the model from overfitting**.\n",
        "- In the context of attention (especially in transformers), there is a similar effect called **attention-dropout**: **after** **softmax**, we obtain attention weights, and with some probability, some of these weights are set to zero. This makes the attention distribution \"noisy\" during training, which helps:\n",
        "  - to prevent models from overfitting on the same keys/values,\n",
        "  - to force attention to be distributed more evenly, which results in better generalization.\n",
        "- In the official attention function in PyTorch, the dropout_p parameter is responsible for this operation: if dropout_p > 0.0, dropout is applied to the attention weights.\n",
        "\n",
        "By Pytroch [scaled_dot_product_attention documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html):\n",
        "\n",
        "> Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified.\n",
        "\n",
        "That is, if `dropout_p > 0.0`, dropout is applied at the softmax weights stage. If `dropout_p = 0.0`, dropout is not applied.\n",
        "\n",
        "Also, the PyTorch comments recommend using `dropout_p = 0.0` for **inference/eval** to **prevent accidental inference**.\n"
      ],
      "metadata": {
        "id": "wLI9rgz1Hkmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def scaled_dot_product_attention_custom(\n",
        "  q, k, v,\n",
        "  attn_mask=None,\n",
        "  is_causal=False,\n",
        "  dropout_p=0.0,\n",
        "):\n",
        "\n",
        "  # API CONTRACT (PyTorch is not allow use is_causal=True and attn_mask)\n",
        "  if is_causal and attn_mask is not None:\n",
        "    raise RuntimeError(\n",
        "      \"Explicit attn_mask should not be set when is_causal=True\"\n",
        "    )\n",
        "\n",
        "  d_k = k.size(-1)\n",
        "\n",
        "  # Step 1: Calculate Similarity\n",
        "  scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "  # Step 2: Scaling\n",
        "  scores = scores / math.sqrt(d_k) # in Pytorch: scores / math.sqrt(d_k)\n",
        "\n",
        "  # Step 3a: Masking (if provided)\n",
        "  if is_causal:\n",
        "    L = q.size(-2)\n",
        "    causal_mask = torch.triu(\n",
        "      torch.ones(L, L, device=q.device, dtype=torch.bool),\n",
        "      diagonal=1 # True above the diagonal; Diagonal itself is NOT touched\n",
        "    )\n",
        "    # If causal_mask[t, j] == True: scores[t, j] = -âˆž -> softmax(-âˆž) = 0\n",
        "    scores = scores.masked_fill(causal_mask, float('-inf'))\n",
        "\n",
        "  # Step 3b: attention mask (padding / custom)\n",
        "  if attn_mask is not None:\n",
        "    scores = scores.masked_fill(~attn_mask, float('-inf'))\n",
        "\n",
        "  # Step 4: Softmax -> attention weights\n",
        "  attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "  # Step 5: Dropout\n",
        "  if dropout_p > 0.0:\n",
        "    attn_weights = torch.dropout(attn_weights, dropout_p, train=True)\n",
        "\n",
        "  # Step 6: Weighted sum of values\n",
        "  output = torch.matmul(attn_weights, v)\n",
        "\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "XeTLdi277U3b"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test dropout\n",
        "\n",
        "The values â€‹â€‹will differ slightly due to stochastic filtering"
      ],
      "metadata": {
        "id": "5uswt2wzj3e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "B, L, D = 2, 6, 8\n",
        "q = torch.randn(B, L, D)\n",
        "k = torch.randn(B, L, D)\n",
        "v = torch.randn(B, L, D)\n",
        "\n",
        "padding_mask = torch.tensor([\n",
        "    [1,1,1,0,0,0],\n",
        "    [1,1,1,1,0,0]\n",
        "], dtype=torch.bool)\n",
        "attn_mask = padding_mask[:, None, :]\n",
        "\n",
        "out_custom = scaled_dot_product_attention_custom(\n",
        "    q, k, v,\n",
        "    attn_mask=attn_mask,\n",
        "    dropout_p=0.1,\n",
        "    is_causal=False\n",
        ")\n",
        "\n",
        "out_torch = F.scaled_dot_product_attention(\n",
        "    q, k, v,\n",
        "    attn_mask=attn_mask,\n",
        "    dropout_p=0.1,\n",
        "    is_causal=False\n",
        ")\n",
        "# Values may differ slightly due to stochastic dropout during training\n",
        "print(\"Custom output with dropout:\\n\", out_custom)\n",
        "print(\"PyTorch output with dropout:\\n\", out_torch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LKMF0HckIE0",
        "outputId": "ac293b8e-f6ee-4c52-90b8-3c753fb96800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom output with dropout:\n",
            " tensor([[[ 9.5632e-01, -1.4926e-01,  9.2037e-01,  8.6204e-01, -9.0496e-01,\n",
            "           5.9019e-01, -2.8997e-01, -1.1777e+00],\n",
            "         [ 5.7533e-01,  1.8178e-01,  6.7345e-01,  5.9659e-01, -7.1458e-01,\n",
            "           7.9585e-01, -3.6383e-01, -5.3604e-01],\n",
            "         [-1.8156e-01,  3.1829e-02,  1.7305e-01,  1.9385e-01,  1.3330e-01,\n",
            "           1.2359e-01, -1.4741e-02,  2.4852e-01],\n",
            "         [ 7.0718e-01, -3.4557e-01,  8.1082e-01,  8.1080e-01, -5.4638e-01,\n",
            "           2.1007e-01, -9.5922e-02, -1.0049e+00],\n",
            "         [ 3.8729e-01, -3.5764e-01,  6.9743e-01,  7.3303e-01, -2.2811e-01,\n",
            "           5.9353e-02,  1.1549e-03, -6.3583e-01],\n",
            "         [ 8.1055e-01, -2.5410e-01,  8.5741e-01,  8.3156e-01, -7.0109e-01,\n",
            "           3.8186e-01, -1.8278e-01, -1.0705e+00]],\n",
            "\n",
            "        [[-5.7556e-01, -5.4034e-02,  2.8969e-01, -7.2152e-01,  3.7089e-01,\n",
            "          -8.4577e-01,  1.1345e+00,  8.5560e-01],\n",
            "         [-5.3907e-02,  7.4537e-01,  3.3100e-01, -3.2346e-01, -1.9480e-01,\n",
            "          -8.2233e-01,  1.0187e+00,  1.1309e+00],\n",
            "         [-1.0932e+00, -9.4227e-01,  2.8119e-01, -9.5743e-01,  8.5412e-01,\n",
            "          -2.7741e-01,  1.0003e+00,  1.8491e-01],\n",
            "         [-4.2478e-01,  2.6835e-01,  5.9796e-01, -6.6653e-01, -1.6010e-01,\n",
            "          -2.1294e-01,  1.5134e+00,  3.9679e-01],\n",
            "         [-3.7624e-01,  1.2972e-03,  2.6652e-01, -6.2245e-01,  1.6352e-01,\n",
            "          -7.1127e-01,  1.0695e+00,  5.3067e-01],\n",
            "         [-3.0555e-01,  7.3394e-01,  7.0269e-01, -5.9442e-01, -3.5868e-01,\n",
            "          -3.7189e-01,  1.6874e+00,  8.5920e-01]]])\n",
            "PyTorch output with dropout:\n",
            " tensor([[[ 9.5632e-01, -1.4926e-01,  9.2037e-01,  8.6204e-01, -9.0496e-01,\n",
            "           5.9019e-01, -2.8997e-01, -1.1777e+00],\n",
            "         [ 5.5459e-01, -1.2266e-01,  7.8272e-01,  7.6230e-01, -5.2998e-01,\n",
            "           4.5962e-01, -1.9452e-01, -6.8855e-01],\n",
            "         [ 6.1639e-01,  9.8261e-02,  8.2947e-01,  7.6643e-01, -7.2091e-01,\n",
            "           7.9600e-01, -3.5179e-01, -6.2572e-01],\n",
            "         [ 7.0718e-01, -3.4557e-01,  8.1082e-01,  8.1080e-01, -5.4638e-01,\n",
            "           2.1007e-01, -9.5922e-02, -1.0049e+00],\n",
            "         [ 3.8729e-01, -3.5764e-01,  6.9743e-01,  7.3303e-01, -2.2811e-01,\n",
            "           5.9353e-02,  1.1549e-03, -6.3583e-01],\n",
            "         [ 8.3901e-01, -2.8124e-01,  8.3671e-01,  8.1149e-01, -7.0981e-01,\n",
            "           3.3730e-01, -1.6818e-01, -1.1225e+00]],\n",
            "\n",
            "        [[-5.7556e-01, -5.4034e-02,  2.8969e-01, -7.2152e-01,  3.7089e-01,\n",
            "          -8.4577e-01,  1.1345e+00,  8.5560e-01],\n",
            "         [-6.7042e-01,  1.2902e-02,  3.6278e-01, -7.7346e-01,  4.0611e-01,\n",
            "          -8.6847e-01,  1.2594e+00,  1.0603e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "         [-5.4351e-02,  7.0844e-01,  5.7886e-01, -3.9616e-01, -5.2115e-01,\n",
            "          -1.8522e-01,  1.3688e+00,  4.3920e-01],\n",
            "         [-5.4106e-02,  3.8402e-01,  2.4991e-01, -3.8733e-01, -1.5046e-01,\n",
            "          -6.8716e-01,  9.4374e-01,  5.6755e-01],\n",
            "         [-3.0847e-01,  6.7436e-01,  6.6585e-01, -6.2130e-01, -3.1518e-01,\n",
            "          -4.5783e-01,  1.6767e+00,  8.4472e-01]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head"
      ],
      "metadata": {
        "id": "-z708kE3lv_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Runs multiple attention mechanisms in parallel\n",
        "- Allows the model to jointly attend to information from different representation subspaces at different positions. Each head can potentially learn to focus on different types of relationships or features.\n",
        "\n",
        "**Multi-Head Workflow**\n",
        "1. Determine the number of heads: `num_heads`\n",
        "2. Divide the embedding `d_model -> (num_heads, head_dim)`\n",
        "3. Parallelize SDPA across all heads simultaneously\n",
        "4. Broadcast masks on `[B, num_heads, L, L]`\n",
        "5. Softmax + dropout for each head\n",
        "6. Concatenate and linearly transform back to `d_model`"
      ],
      "metadata": {
        "id": "WaioTlPul72D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does PyTorch nn.MultiheadAttention return?\n",
        "\n",
        "https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html?spm=a2ty_o01.29997173.0.0.38985171kySQjU\n",
        "\n",
        "\n",
        "```python\n",
        "# PyTorch returns:\n",
        "# output: [batch_size, seq_len, embed_dim]\n",
        "# attn_weights: [batch_size, seq_len, seq_len] (averaged across heads by default)\n",
        "# or if average_attn_weights=False: [batch_size, num_heads, seq_len, seq_len]\n",
        "```"
      ],
      "metadata": {
        "id": "sGvpCzLvOhyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial parameters:\n",
        "batch_size = 2 # B\n",
        "seq_len = 5 # L\n",
        "embed_dim = 512 # d_model\n",
        "num_heads = 8 # H\n",
        "\n",
        "# After head splitting:\n",
        "head_dim = embed_dim // num_heads # 512 / 8 = 64\n",
        "\n",
        "# Q, K, V before splitting: [B, L, d_model]\n",
        "# After splitting: [B, L, H, head_dim]\n",
        "# After transpose: [B, H, L, head_dim]"
      ],
      "metadata": {
        "id": "WjT5WJ0_-GEK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Basic Structure of MultiHeadAttention"
      ],
      "metadata": {
        "id": "b85v7etmG7Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads=1, dropout=0.0):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      embed_dim: total embedding dimension (d_model)\n",
        "      num_heads: number of attention heads\n",
        "      dropout: dropout probability\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    # Check: embed_dim must be divisible by num_heads\n",
        "    assert embed_dim % num_heads == 0, \\\n",
        "      f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\"\n",
        "\n",
        "    # Size per head\n",
        "    self.head_dim = embed_dim // num_heads\n",
        "\n",
        "    # Linear layers for Q, K, V\n",
        "    self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "    self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "    self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    # Final Linear Layer\n",
        "    self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, query, key, value, attn_mask=None, is_causal=False,\n",
        "              need_weights=False, average_attn_weights=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      query: [batch_size, seq_len, embed_dim]\n",
        "      key: [batch_size, seq_len, embed_dim]\n",
        "      value: [batch_size, seq_len, embed_dim]\n",
        "      attn_mask: optional mask [batch_size, seq_len, seq_len] or [batch_size, num_heads, seq_len, seq_len]\n",
        "      is_causal: ÐµÑÐ»Ð¸ True, Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ÑÑ causal mask\n",
        "      need_weights: If True, maintains attention weights.\n",
        "      average_attn_weights: If True and need_weights=True, averages weights across heads.\n",
        "\n",
        "    Returns:\n",
        "      output: [batch_size, seq_len, embed_dim]\n",
        "      attn_weights: if need_weights=True:\n",
        "        - if average_attn_weights=True: [batch_size, seq_len, seq_len]\n",
        "        - if average_attn_weights=False: [batch_size, num_heads, seq_len, seq_len]\n",
        "    \"\"\"\n",
        "    batch_size = query.size(0)\n",
        "\n",
        "    # 1. Linear transformations\n",
        "    Q = self.q_proj(query)  # [B, L, d_model]\n",
        "    K = self.k_proj(key)    # [B, L, d_model]\n",
        "    V = self.v_proj(value)  # [B, L, d_model]\n",
        "\n",
        "    # 2. Break it down into heads\n",
        "    Q = self._split_heads(Q)  # [B, H, L, head_dim]\n",
        "    K = self._split_heads(K)  # [B, H, L, head_dim]\n",
        "    V = self._split_heads(V)  # [B, H, L, head_dim]\n",
        "\n",
        "    # 3. Calculate the scaled dot-product attention for all heads\n",
        "    attn_output, attn_weights = scaled_dot_product_attention(\n",
        "      Q, K, V,\n",
        "      attn_mask,\n",
        "      is_causal,\n",
        "      dropout_p=self.dropout,\n",
        "      training=self.training\n",
        "    )\n",
        "\n",
        "    # 4. Put the heads back together\n",
        "    attn_output = self._combine_heads(attn_output)  # [B, L, d_model]\n",
        "\n",
        "    # 5. Final linear transformation\n",
        "    output = self.out_proj(attn_output)\n",
        "\n",
        "    # 6. Handling weights like in PyTorch\n",
        "    if need_weights:\n",
        "      if average_attn_weights:\n",
        "        # Averaging over heads: [B, H, L, L] -> [B, L, L]\n",
        "        attn_weights = attn_weights.mean(dim=1)\n",
        "      return output, attn_weights\n",
        "    else:\n",
        "      return output, None  # Or just output if you want an exact match with PyTorch\n",
        "\n",
        "  def _split_heads(self, x):\n",
        "    \"\"\"\n",
        "    Splits the tensor into heads\n",
        "    [B, L, d_model] -> [B, H, L, head_dim]\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, _ = x.size()\n",
        "\n",
        "    # Reshape: [B, L, H, head_dim]\n",
        "    x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "    # Transpose: [B, H, L, head_dim] (for convenience matmul)\n",
        "    x = x.transpose(1, 2)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def _combine_heads(self, x):\n",
        "    \"\"\"\n",
        "    Puts the heads back together\n",
        "    [B, H, L, head_dim] -> [B, L, d_model]\n",
        "    \"\"\"\n",
        "    batch_size, num_heads, seq_len, head_dim = x.size()\n",
        "\n",
        "    # Transpose: [B, L, H, head_dim]\n",
        "    x = x.transpose(1, 2)\n",
        "\n",
        "    # Reshape: [B, L, d_model]\n",
        "    x = x.contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "-q4IgvAtG8-i"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Modify scaled_dot_product_attention for Multi-Head\n",
        "\n",
        "Single-Head Attention:\n",
        "```python\n",
        "# Q, K, V: [batch, seq_len, d_k]\n",
        "d_k = k.size(-1) # for example, 512\n",
        "# d_k - This is the full embedding dimension.\n",
        "scores = scores / math.sqrt(d_k)  # devide to sqrt(512)\n",
        "```\n",
        "\n",
        "Multi-Head Attention:\n",
        "```python\n",
        "# Q, K, V: [batch, seq_len, d_k]\n",
        "d_k = k.size(-1) # for example, 64 (512 / 8 heads)\n",
        "# d_k - This is the full embedding dimension.\n",
        "scores = scores / math.sqrt(d_k)  # devide to sqrt(64)\n",
        "```\n",
        "\n",
        "In the original article \"Attention Is All You Need\" the formula is:\n",
        "\n",
        "`Attention(Q, K, V) =  softmax(QÂ·Káµ€/âˆšd_k)Â·V`\n",
        "\n",
        "But in Multi-Head:\n",
        "- Each head handles a portion of the embedding.\n",
        "- `d_k` in the formula refers to the KEY dimension in **ONE HEAD**.\n",
        "- Not to the full embedding dimension!\n",
        "\n",
        "Masking also changed and applyed for each head:\n",
        "\n",
        "Single-Head Attention:\n",
        "```python\n",
        "# mask = [seq_len, seq_len] or [batch_size, seq_len, seq_len]\n",
        "\n",
        "if is_causal:\n",
        "  causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "  scores = scores.masked_fill(causal_mask, float('-inf'))\n",
        "```\n",
        "\n",
        "Multi-Head Attention:\n",
        "```python\n",
        "# mask = [batch_size, num_heads, seq_len, seq_len]\n",
        "\n",
        "if is_causal:\n",
        "  causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "  # 1. Add batch and head dimensions: [1, 1, seq_len, seq_len]\n",
        "  causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n",
        "  # 2. Expand to batch_size and num_heads: [batch_size, num_heads, seq_len, seq_len]\n",
        "  causal_mask = causal_mask.expand(batch_size, num_heads, -1, -1)\n",
        "            \n",
        "  scores = scores.masked_fill(causal_mask, float('-inf'))\n",
        "```\n",
        "\n",
        "As about **attention mask**\n",
        "\n",
        "For Single-Head\n",
        "- dim=2: [L, L] -> expand to [B, L, L]\n",
        "- dim=3: [B, L, L] is already done\n",
        "```python\n",
        "if attn_mask.dim() == 2:          # [L, L]\n",
        "  attn_mask = attn_mask.unsqueeze(0)  # [1, L, L]\n",
        "  attn_mask = attn_mask.expand(batch_size, -1, -1)\n",
        "elif attn_mask.dim() == 3:        # [B, L, L]\n",
        "  pass\n",
        "```\n",
        "\n",
        "Multi-Head:\n",
        "- dim=2: [L, L] -> expand to [B, H, L, L]\n",
        "- dim=3: [B, L, L] -> expand to [B, H, L, L]\n",
        "- dim=4: [B, H, L, L] is already done\n",
        "```python\n",
        "if attn_mask.dim() == 3:  # [B, L, L]\n",
        "    attn_mask = attn_mask.unsqueeze(1)  # [B, 1, L, L]\n",
        "    attn_mask = attn_mask.expand(-1, num_heads, -1, -1)\n",
        "elif attn_mask.dim() == 2:  # [L, L]\n",
        "    attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, L, L]\n",
        "    attn_mask = attn_mask.expand(batch_size, num_heads, -1, -1)\n",
        "elif attn_mask.dim() == 4:  # [B, H, L, L]\n",
        "    pass\n",
        "```"
      ],
      "metadata": {
        "id": "ny2G9gIEKpv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(\n",
        "  q, k, v,\n",
        "  attn_mask=None,\n",
        "  is_causal=False,\n",
        "  dropout_p=0.0,\n",
        "  training=True\n",
        "  ):\n",
        "  \"\"\"\n",
        "  Scaled dot-product attention Ð´Ð»Ñ multi-head\n",
        "  Args:\n",
        "    Q: [B, H, L, head_dim]\n",
        "    K: [B, H, L, head_dim]\n",
        "    V: [B, H, L, head_dim]\n",
        "  Returns:\n",
        "    output: [B, H, L, head_dim]\n",
        "    attn_weights: [B, H, L, L]\n",
        "  \"\"\"\n",
        "  # API CONTRACT (PyTorch is not allow use is_causal=True and attn_mask)\n",
        "  if is_causal and attn_mask is not None:\n",
        "    raise RuntimeError(\n",
        "      \"Explicit attn_mask should not be set when is_causal=True\"\n",
        "    )\n",
        "\n",
        "  # Determine the mode: single-head or multi-head\n",
        "  is_multi_head = (q.dim() == 4)\n",
        "\n",
        "  if is_multi_head:\n",
        "    batch_size, num_heads, seq_len, head_dim = q.size()\n",
        "  else:  # single-head\n",
        "    batch_size, seq_len, d_k = q.size()\n",
        "    num_heads = 1  # Ð´Ð»Ñ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð°\n",
        "\n",
        "  # 1. Calculate similarity (Q * K^T)\n",
        "  scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "  # 2. Scaling\n",
        "  scale_dim = q.size(-1) # d_k for single-head, head_dim for multi-head\n",
        "  scores = scores / math.sqrt(scale_dim)\n",
        "\n",
        "  # 3. Causal mask\n",
        "  if is_causal:\n",
        "    # Create a causal mask for each head\n",
        "    causal_mask = torch.triu(\n",
        "      torch.ones(seq_len, seq_len, device=q.device, dtype=torch.bool),\n",
        "      diagonal=1\n",
        "    )\n",
        "\n",
        "    # Adapt the dimension to the input\n",
        "    if is_multi_head:\n",
        "      # Expand for batch and heads: [1, 1, L, L] -> [B, H, L, L]\n",
        "      causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n",
        "      causal_mask = causal_mask.expand(batch_size, num_heads, -1, -1)\n",
        "    else:\n",
        "      # [1, L, L] -> [B, L, L]\n",
        "      causal_mask = causal_mask.unsqueeze(0)\n",
        "      causal_mask = causal_mask.expand(batch_size, -1, -1)\n",
        "\n",
        "    scores = scores.masked_fill(causal_mask, float('-inf'))\n",
        "\n",
        "  # 4. Attention mask\n",
        "  if attn_mask is not None:\n",
        "    # Check and adapt the mask size\n",
        "    if is_multi_head:\n",
        "      if attn_mask.dim() == 3:  # [B, L, L]\n",
        "        attn_mask = attn_mask.unsqueeze(1)  # [B, 1, L, L]\n",
        "        attn_mask = attn_mask.expand(-1, num_heads, -1, -1)\n",
        "      elif attn_mask.dim() == 2:  # [L, L]\n",
        "        attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, L, L]\n",
        "        attn_mask = attn_mask.expand(batch_size, num_heads, -1, -1)\n",
        "      elif attn_mask.dim() == 4:  # [B, H, L, L]\n",
        "        pass\n",
        "      else:\n",
        "        raise ValueError(f\"Invalid attn_mask dim for multi-head: {attn_mask.dim()}\")\n",
        "\n",
        "    else: # single-head\n",
        "      if attn_mask.dim() == 2:          # [L, L]\n",
        "        attn_mask = attn_mask.unsqueeze(0)  # [1, L, L]\n",
        "        attn_mask = attn_mask.expand(batch_size, -1, -1)\n",
        "      elif attn_mask.dim() == 3:        # [B, L, L]\n",
        "        pass\n",
        "      else:\n",
        "        raise ValueError(f\"Invalid attn_mask dim for multi-head: {attn_mask.dim()}\")\n",
        "\n",
        "    # PyTorch semantics: True = allowed, False = masked\n",
        "    scores = scores.masked_fill(~attn_mask, float('-inf'))\n",
        "\n",
        "  # 5. Softmax\n",
        "  attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "  # 6. Dropout\n",
        "  if dropout_p > 0.0 and training:\n",
        "    #attn_weights = torch.dropout(attn_weights, dropout_p)\n",
        "    attn_weights = F.dropout(attn_weights, dropout_p)\n",
        "\n",
        "  # 7. Weighted sum\n",
        "  output = torch.matmul(attn_weights, v)  # [B, H, L, head_dim]\n",
        "\n",
        "  return output, attn_weights"
      ],
      "metadata": {
        "id": "_bVDvEWyKscv"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only bool attn_mask with True = allowed is supported"
      ],
      "metadata": {
        "id": "WkaC5CEmgmt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for Multihead"
      ],
      "metadata": {
        "id": "Da66izM4yQcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multihead_simple():\n",
        "  \"\"\"A simple test for understanding dimensions\"\"\"\n",
        "  print(\"ðŸ§ª Testing MultiHeadAttention dimensions\")\n",
        "\n",
        "  # Parameters\n",
        "  batch_size = 2\n",
        "  seq_len = 5\n",
        "  embed_dim = 512\n",
        "  num_heads = 8\n",
        "\n",
        "  # Create the model\n",
        "  mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "\n",
        "  # Test data\n",
        "  query = torch.randn(batch_size, seq_len, embed_dim)\n",
        "  key = torch.randn(batch_size, seq_len, embed_dim)\n",
        "  value = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "  # Forward pass\n",
        "  output, attn_weights = mha(query, key, value, need_weights=True)\n",
        "\n",
        "  print(f\"Input query shape: {query.shape}\")\n",
        "  print(f\"Output shape: {output.shape}\")\n",
        "  print(f\"Attention weights shape: {attn_weights.shape}\")\n",
        "\n",
        "  # Checking dimensions\n",
        "  assert output.shape == (batch_size, seq_len, embed_dim), \\\n",
        "    f\"Expected output shape {(batch_size, seq_len, embed_dim)}, got {output.shape}\"\n",
        "\n",
        "  assert attn_weights.shape == (batch_size, seq_len, seq_len), \\\n",
        "        f\"Expected weights shape {(batch_size, seq_len, seq_len)}, got {attn_weights.shape}\"\n",
        "\n",
        "  print(\"âœ… All dimensions are correct!\")\n",
        "\n",
        "  # Visualizing one head\n",
        "  print(f\"\\nðŸ‘ï¸ Example of an attention matrix (head 0, batch 0):\")\n",
        "  print(attn_weights[0, 0].detach().numpy().round(3))\n",
        "\n",
        "  return output, attn_weights\n",
        "\n",
        "test_multihead_simple()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7fThu0byVBS",
        "outputId": "6d5e2be0-6540-4185-8e83-1bd7d347e491"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Testing MultiHeadAttention dimensions\n",
            "Input query shape: torch.Size([2, 5, 512])\n",
            "Output shape: torch.Size([2, 5, 512])\n",
            "Attention weights shape: torch.Size([2, 5, 5])\n",
            "âœ… All dimensions are correct!\n",
            "\n",
            "ðŸ‘ï¸ Example of an attention matrix (head 0, batch 0):\n",
            "[0.224 0.198 0.195 0.186 0.197]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.1155, -0.1097,  0.1715,  ..., -0.0114, -0.0032, -0.0264],\n",
              "          [ 0.1867, -0.1763,  0.2732,  ..., -0.0570,  0.1043, -0.1021],\n",
              "          [ 0.1622, -0.1501,  0.2313,  ..., -0.0567,  0.0152, -0.0091],\n",
              "          [ 0.0699, -0.1284,  0.2175,  ...,  0.0474,  0.0816, -0.0261],\n",
              "          [ 0.1811, -0.0816,  0.2691,  ..., -0.0685,  0.0048,  0.0786]],\n",
              " \n",
              "         [[ 0.1004, -0.1032,  0.1101,  ...,  0.0930, -0.0176, -0.0471],\n",
              "          [ 0.0747, -0.0830,  0.0581,  ...,  0.1827,  0.0425,  0.0497],\n",
              "          [ 0.0932, -0.0439,  0.1023,  ...,  0.1871, -0.0284,  0.0468],\n",
              "          [ 0.0527, -0.0408,  0.0999,  ...,  0.0185,  0.0605,  0.0455],\n",
              "          [ 0.0853, -0.0938,  0.0902,  ...,  0.1078,  0.0175,  0.0066]]],\n",
              "        grad_fn=<ViewBackward0>),\n",
              " tensor([[[0.2241, 0.1982, 0.1951, 0.1860, 0.1966],\n",
              "          [0.1781, 0.2107, 0.2200, 0.1852, 0.2061],\n",
              "          [0.2072, 0.1958, 0.1532, 0.1889, 0.2548],\n",
              "          [0.2040, 0.2107, 0.2155, 0.1605, 0.2092],\n",
              "          [0.1800, 0.2434, 0.1852, 0.1783, 0.2132]],\n",
              " \n",
              "         [[0.1844, 0.1872, 0.2354, 0.2046, 0.1883],\n",
              "          [0.2484, 0.1924, 0.1976, 0.1965, 0.1651],\n",
              "          [0.1796, 0.1740, 0.2505, 0.2117, 0.1841],\n",
              "          [0.1900, 0.2108, 0.1878, 0.1919, 0.2194],\n",
              "          [0.2106, 0.1903, 0.1930, 0.2128, 0.1932]]], grad_fn=<MeanBackward1>))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare with Pytorch realisation"
      ],
      "metadata": {
        "id": "pe410Zi0zoKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def test_multihead_vs_pytorch():\n",
        "  \"\"\"\n",
        "  Comparing our MultiHeadAttention implementation with PyTorch nn.MultiheadAttention\n",
        "  \"\"\"\n",
        "  print(\"ðŸ§ª Testing MultiHeadAttention against PyTorch\")\n",
        "  print(\"=\"*60)\n",
        "\n",
        "  # Test parameters\n",
        "  torch.manual_seed(42)\n",
        "  batch_size = 2\n",
        "  seq_len = 6\n",
        "  embed_dim = 64 # must be divisible by num_heads\n",
        "  num_heads = 4\n",
        "\n",
        "  # Check divisibility\n",
        "  assert embed_dim % num_heads == 0, f\"embed_dim {embed_dim} must be divisible by num_heads {num_heads}\"\n",
        "\n",
        "  # 1. Create test data\n",
        "  query = torch.randn(batch_size, seq_len, embed_dim)\n",
        "  key = torch.randn(batch_size, seq_len, embed_dim)\n",
        "  value = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "  print(f\"Test parameters:\")\n",
        "  print(f\" batch_size: {batch_size}\")\n",
        "  print(f\" seq_len: {seq_len}\")\n",
        "  print(f\" embed_dim: {embed_dim}\")\n",
        "  print(f\" num_heads: {num_heads}\")\n",
        "  print(f\" head_dim: {embed_dim // num_heads}\")\n",
        "\n",
        "  # 2. PyTorch MultiheadAttention\n",
        "  # WARNING: PyTorch expects [seq_len, batch_size, embed_dim]\n",
        "  pytorch_mha = nn.MultiheadAttention(\n",
        "    embed_dim=embed_dim,\n",
        "    num_heads=num_heads,\n",
        "    batch_first=True, # important! for [batch, seq, embed]\n",
        "    dropout=0.0 # for deterministic comparison\n",
        "  )\n",
        "\n",
        "  # Switch to eval mode (disable dropout)\n",
        "  pytorch_mha.eval()\n",
        "\n",
        "  #3. Our implementation of MultiHeadAttention\n",
        "  our_mha = MultiHeadAttention(embed_dim, num_heads, dropout=0.0)\n",
        "\n",
        "  # Copy weights from PyTorch to our model for accurate comparison\n",
        "  with torch.no_grad():\n",
        "    # Copy weights from PyTorch\n",
        "    our_mha.q_proj.weight.copy_(pytorch_mha.in_proj_weight[:embed_dim, :])\n",
        "    our_mha.q_proj.bias.copy_(pytorch_mha.in_proj_bias[:embed_dim])\n",
        "    our_mha.k_proj.weight.copy_(pytorch_mha.in_proj_weight[embed_dim:2*embed_dim, :])\n",
        "    our_mha.k_proj.bias.copy_(pytorch_mha.in_proj_bias[embed_dim:2*embed_dim])\n",
        "    our_mha.v_proj.weight.copy_(pytorch_mha.in_proj_weight[2*embed_dim:, :])\n",
        "    our_mha.v_proj.bias.copy_(pytorch_mha.in_proj_bias[2*embed_dim:])\n",
        "    our_mha.out_proj.weight.copy_(pytorch_mha.out_proj.weight)\n",
        "    our_mha.out_proj.bias.copy_(pytorch_mha.out_proj.bias)\n",
        "\n",
        "    # Test 1\n",
        "    print(\"\\n1. Test Output comparison:\")\n",
        "    with torch.no_grad():\n",
        "        pytorch_output = pytorch_mha(query, key, value, need_weights=False)[0]\n",
        "\n",
        "    our_output = our_mha(query, key, value, need_weights=False)[0]\n",
        "\n",
        "    output_diff = (our_output - pytorch_output).abs().max().item()\n",
        "    print(f\"  Output diff: {output_diff:.2e}\")\n",
        "    print(f\"  Outputs match: {'âœ…' if output_diff < 1e-6 else 'âŒ'}\")\n",
        "\n",
        "    # Test 2\n",
        "    print(\"\\n2. Test with averaged weights:\")\n",
        "    with torch.no_grad():\n",
        "        pytorch_output, pytorch_weights = pytorch_mha(\n",
        "            query, key, value,\n",
        "            need_weights=True,\n",
        "            average_attn_weights=True\n",
        "        )\n",
        "\n",
        "    our_output, our_weights = our_mha(\n",
        "        query, key, value,\n",
        "        need_weights=True,\n",
        "        average_attn_weights=True\n",
        "    )\n",
        "\n",
        "    weights_diff = (our_weights - pytorch_weights).abs().max().item()\n",
        "    print(f\"  Weights diff: {weights_diff:.2e}\")\n",
        "    print(f\"  Weights match: {'âœ…' if weights_diff < 1e-6 else 'âŒ'}\")\n",
        "\n",
        "    # Test 3\n",
        "    print(\"\\n3. Per-head weights comparison:\")\n",
        "    with torch.no_grad():\n",
        "      pytorch_output, pytorch_weights = pytorch_mha(\n",
        "        query, key, value,\n",
        "        need_weights=True,\n",
        "        average_attn_weights=False\n",
        "      )\n",
        "\n",
        "    our_output, our_weights = our_mha(\n",
        "      query, key, value,\n",
        "      need_weights=True,\n",
        "      average_attn_weights=False\n",
        "    )\n",
        "\n",
        "    weights_diff = (our_weights - pytorch_weights).abs().max().item()\n",
        "    print(f\"  Per-head weights diff: {weights_diff:.2e}\")\n",
        "    print(f\"  Per-head weights match: {'âœ…' if weights_diff < 1e-6 else 'âŒ'}\")\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ {'ALL TESTS PASSED!' if output_diff < 1e-6 and weights_diff < 1e-6 else 'SOME TESTS FAILED'}\")\n",
        "    return output_diff < 1e-6 and weights_diff < 1e-6\n",
        "\n",
        "    return True\n",
        "\n",
        "test_multihead_vs_pytorch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8npSRSsztBw",
        "outputId": "144be8dd-0d02-42b1-9835-cf2a3d216db9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Testing MultiHeadAttention against PyTorch\n",
            "============================================================\n",
            "Test parameters:\n",
            " batch_size: 2\n",
            " seq_len: 6\n",
            " embed_dim: 64\n",
            " num_heads: 4\n",
            " head_dim: 16\n",
            "\n",
            "1. Test Output comparison:\n",
            "  Output diff: 8.94e-08\n",
            "  Outputs match: âœ…\n",
            "\n",
            "2. Test with averaged weights:\n",
            "  Weights diff: 0.00e+00\n",
            "  Weights match: âœ…\n",
            "\n",
            "3. Per-head weights comparison:\n",
            "  Per-head weights diff: 0.00e+00\n",
            "  Per-head weights match: âœ…\n",
            "\n",
            "ðŸŽ‰ ALL TESTS PASSED!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional"
      ],
      "metadata": {
        "id": "mE1HTDRN-lmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of Attentions\n",
        "\n",
        "### Self-Attention\n",
        "- Keys, queries, and values all come from the same source sequence\n",
        "- Allows each position to attend to all positions in the sequence\n",
        "\n",
        "### Cross-Attention\n",
        "- The queries come from one sequence (e.g., the decoder in a seq2seq model), while the keys and values come from another (e.g., the encoder).\n",
        "- Often used in machine translation and generative tasks where one sequence attends to another.\n",
        "\n",
        "### Multi-Head Attention\n",
        "- Runs multiple attention mechanisms in parallel\n",
        "- Allows the model to jointly attend to information from different representation subspaces at different positions. Each head can potentially learn to focus on different types of relationships or features.\n",
        "\n",
        "### MultiQuery Attention (MQA)\n",
        "- All query heads share the same key and value matrices, only query matrices are different\n",
        "- Significantly reduces memory requirements and inference time\n",
        "- Can lead to quality degradation compared to MHA\n",
        "\n",
        "### Grouped-Query Attention (GQA)\n",
        "- Introduced to balance the efficiency of MQA and the quality in MHA\n",
        "- In MHA, each query head has its own key-value heads (maximum quality but high memory usage). In MQA, all query heads share just one key-value head (maximum efficiency but lower quality). GQA divides query heads into groups, where each group shares a set of key-value heads.\n",
        "- The number of groups (G) is a hyperparameter - more groups is closer to MHA, fewer is closer to MQA\n",
        "- Used in models like Llama 2-70B, Mistral 7B, and Falcon 40B. Particularly useful in multi-GPU environments with tensor parallelism\n",
        "\n",
        "### Global vs. Local Attention\n",
        "- Global Attention attends to all positions in the sequence (standard approach). It helps maintain long-range dependencies that local attention might miss.\n",
        "- Local Attention attends only to a window of positions around the current position. It reduces computational complexity from O(nÂ²) to O(n).\n",
        "- Architectures like Longformer and BigBird use hybrid approaches combining both: local attention for most tokens, augmented with some form of global attention (specific tokens attending globally, or sparse global attention patterns) to retain the ability to capture long-range dependencies where needed.\n",
        "\n",
        "### Multi-token attention\n",
        "- Addresses limitations of single-token attention where individual weights are determined by similarity of just one query-key pair\n",
        "- Applies convolution operations over queries, keys, and heads to allow neighboring tokens to influence each other's attention weights"
      ],
      "metadata": {
        "id": "7PRVEqGj-oqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resurses\n",
        "\n",
        "* PyTorch Docs: [Dropout](https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
        "* [PyTorch Docs: scaled_dot_product_attention](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n",
        "* [Linformer: Self-Attention with Linear Complexity. (2020 Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma)](https://arxiv.org/abs/2006.04768)\n",
        "* [Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free. (2025 May, Zihan Qiu , Zekun Wang , Bo Zheng , Zeyu Huang , Kaiyue Wen , Songlin Yang , Rui Men , Le Yu , Fei Huang , Suozhi Huang , Dayiheng Liu , Jingren Zhou , Junyang Lin)](https://huggingface.co/papers/2505.06708)\n",
        "* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022 May, Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher RÃ©)](https://arxiv.org/abs/2205.14135)\n",
        "* [Longformer: The Long-Document Transformer (2020, Iz Beltagy, Matthew E. Peters, Arman Cohan)](https://arxiv.org/abs/2004.05150)"
      ],
      "metadata": {
        "id": "kC3NaYgYlPgj"
      }
    }
  ]
}