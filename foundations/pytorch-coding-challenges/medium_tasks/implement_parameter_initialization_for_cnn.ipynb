{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DECYx_h7aaqs"
      },
      "source": [
        "# Implement parameter initialization for a CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "You are tasked with employing and evaluating a CNN model's parameter initialization strategies in Pytorch. Your goal is to initialize the weights and biases of a vanilla CNN model provided in the problem statement and comment on the implications of each strategy."
      ],
      "metadata": {
        "id": "4L12POpZPL2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requirements\n",
        "1. **Initialize** weights and biases in the following ways:\n",
        "   - **Zero Initialization**: set the parameters to zero\n",
        "   - **Random Initialization**: sets model parameters to random values drawn from a normal distribution\n",
        "   - **Xavier Initialization** sets them to random values from a normal distribution with **mean=0 and variance=1/n**\n",
        "   - **Kaiming He Initialization** initializes to random values from a normal distribution with **mean=0 and variance=2/n**\n",
        "2. Train and compute accuracy for each strategy\n",
        "\n",
        "### Constraints\n",
        "   - Use the given CNN model and the training and testing helper functions for accuracy computations.\n",
        "   - Ensure the model is compatible with the CIFAR-10 dataset, which contains 10 classes.\n",
        "\n",
        "**! Hint:**\n",
        "   - Use `torch.nn.init` for weight initialization\n",
        "   - Resources to read: [All you need is a good init](https://arxiv.org/pdf/1511.06422)"
      ],
      "metadata": {
        "id": "r6YxNIKCJBei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Code template"
      ],
      "metadata": {
        "id": "66_mqpqCJD3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMQo9TYHaaqx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the CNN Model\n",
        "# TODO: Add convolutional, pooling, and fully connected layers\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        ...\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CNNModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    for images, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "# Evaluate on the test set\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution"
      ],
      "metadata": {
        "id": "xfOkc3NIscKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rephrase"
      ],
      "metadata": {
        "id": "8Uj1quWeJHJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- initialize the weights and biases of a vanilla CNN model\n",
        "   - Zero Initialization: set the parameters to zero\n",
        "   - Random Initialization: sets model parameters to random values drawn from a normal distribution\n",
        "   - Xavier Initialization sets them to random values from a normal distribution with mean=0 and variance=1/n\n",
        "   - Kaiming He Initialization initializes to random values from a normal distribution with mean=0 and variance=2/n\n",
        "- comment on the implications of each strategy\n",
        "- ensure the model is compatible with the CIFAR-10 dataset, which contains 10 classes.\n",
        "\n",
        "**Note**:\n",
        "- By default, PyTorch uses Xavier initialization implemented via Kaiming code (`a=math.sqrt(5)` in `kaiming_uniform_` is mathematically equivalent to Xavier uniform).\". For details, see https://github.com/pytorch/pytorch/issues/57109)\n",
        "- Weights can be redefined, but not in `__init__` (just defining the architecture), and not in `forward` (only calculate the pass) - but after the model is created, before training."
      ],
      "metadata": {
        "id": "DVX_5OHCJKkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation notes\n",
        "\n",
        "| Strategy | Formulas | Behavior | Expected accuracy |\n",
        "|----------|----------|----------|----------|\n",
        "| Zero Initialization | W = 0, b = 0 | All neurons in a layer are identical -> symmetry is not broken -> gradients are the same -> learning does not occur. | ~10% (guessing) |\n",
        "| Random (Naive) | W ~ N(0, 1) | The symmetry is broken. But: with deep networks -> exploding/vanishing gradients: Var(output) = n_in * Var(weight) * Var(input) -> grows/decays exponentially | Low/unstable |\n",
        "| Xavier (Glorot) | W ~ N(0, 1/n_in) or 1/(n_in + n_out) | Preserves signal variance on forward and backward passes -> Suitable for tanh/sigmoid, but not for ReLU. | Good (but not optimal for ReLU) |\n",
        "| Kaiming (He) | W ~ N(0, 2/n_in) | Takes into account that ReLU zeroes out ~50% of the values ‚Äã‚Äã-> compensates for variance losses. -> Standard for ReLU/CNN. | Best (fast convergence, high accuracy) |\n",
        "\n",
        "`n_in` = `fan_in` = number of input features (e.g. `in_channels * kernel_size¬≤` for `Conv2d`)."
      ],
      "metadata": {
        "id": "tMduERNn9xO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero Initialization Principles\n",
        "\n",
        "- All neurons in a single layer receive the same weights (zeros).\n",
        "- On the first forward pass, they compute the same outputs.\n",
        "- During backpropagation, the gradients for all weights in a single layer will be the same.\n",
        "- As a result, all weights will be updated identically.\n",
        "\n",
        "All neurons in a layer remain symmetrical - they learn the same features and are essentially copies of each other. This completely invalidates the idea of ‚Äã‚Äãhaving multiple neurons in a layer!\n",
        "\n",
        "`W := 0 - lr * grad_W` -> All weights will become equal to `-lr * grad_W` (same value)\n",
        "\n",
        "Sometimes bias is initialized to units or small positive values ‚Äã‚Äãto avoid problems with \"vanishing gradients\"."
      ],
      "metadata": {
        "id": "tHjZpxsMPl7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xavier (Glorot) Initialization Principles\n",
        "\n",
        "- Goal: Make the variance of activations the same at the input and output of each layer, both on the forward and backward passes.\n",
        "- W ~ N(0, œÉ¬≤),\n",
        "  - œÉ¬≤ = 1 / n_in (*Xavier-normal*, fan-in only) **or**\n",
        "  - œÉ¬≤ = 2 / (n_in + n_out) (*Glorot-normal*, default –≤ `xavier_normal_`) - harmonic mean between forward and reverse pass requirements. Suitable when the activation is symmetrical, for example: `tanh`, `sigmoid` (their derivatives are ~1 near 0)\n",
        "  - **`n_in`** = `fan_in` = number of inputs to the neuron (e.g. `3*3*3 = 27` for the first `Conv2d(3 -> 16)`)\n",
        "\n",
        "! **Not for Relu**\n",
        "\n",
        "ReLU zeroes out ~50% of the values ‚Äã‚Äã‚Üí the output variance is half what it should be.\n",
        "\n",
        "**Xavier undercompensates** - the signal still attenuates"
      ],
      "metadata": {
        "id": "np5bnNtC2s7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kaiming (He) Initialization Principles\n",
        "\n",
        "- Compensates for the loss of 50% variance in ReLU\n",
        "- `W ~ N(0, œÉ¬≤)`\n",
        "   - œÉ¬≤ = 2 / n_in\n",
        "- Although at 2025, PyTorch provides kaiming_uniform_() function, the DEFAULT initialization in nn.Conv2d/Linear is actually Xavier-equivalent, not true Kaiming for ReLU (https://github.com/pytorch/pytorch/issues/57109)\n",
        "\n",
        "**Xavier** preserves signal variance for symmetric activations like tanh. **Kaiming** does the same for ReLU - it doubles the weight variance to compensate for ReLU zeroing out half of the values. Therefore, **for modern CNNs with ReLU, Kaiming is the standard**."
      ],
      "metadata": {
        "id": "P1oqZ9hO4_MD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution Code"
      ],
      "metadata": {
        "id": "no3kG1lVBXfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast Code Attempt (base by provided template)"
      ],
      "metadata": {
        "id": "OGkG2nXf1er9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the CNN Model\n",
        "# TODO: Add convolutional, pooling, and fully connected layers\n",
        "class CNNModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "\n",
        "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(32 * 8 * 8, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 10)\n",
        "  )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.network(x)\n",
        "\n",
        "init_strategies = ['zero', 'random']\n",
        "\n",
        "# apply(fn) ‚Äî recursively applies the function fn to all submodules (including self), calling fn(module) for each.\n",
        "def make_initializer(strategy):\n",
        "  def init_fn(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "      if strategy == 'zero':\n",
        "        nn.init.zeros_(m.weight)\n",
        "      elif strategy == 'random':\n",
        "        nn.init.normal_(m.weight, 0.0, 1.0)\n",
        "  return init_fn\n",
        "\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "for init_strategy in init_strategies:\n",
        "  model = CNNModel()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  print(\"\\n\")\n",
        "  print(\"*\"*25, '\\n')\n",
        "  print(f\"*** Strategy: {init_strategy.upper()} ***\\n\")\n",
        "\n",
        "  print(\"* Before redefining * \\n\")\n",
        "  print(\"Part of weights: \\n\\n\", model.network[0].weight[0, 0, :2, :2], \"\\n\")\n",
        "  print(\"Sum of weights: \", model.network[0].weight.sum())\n",
        "  print(\"\\n\")\n",
        "\n",
        "  model.apply(make_initializer(init_strategy))\n",
        "\n",
        "  print(\"* After redefining * \\n\")\n",
        "  print(\"Part of weights: \\n\\n\", model.network[0].weight[0, 0, :2, :2], \"\\n\")\n",
        "  print(\"Sum of weights: \", model.network[0].weight.sum())\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"*** Training \\n\")\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  # Training loop\n",
        "  epochs = 10\n",
        "  for epoch in range(epochs):\n",
        "    for images, labels in train_loader:\n",
        "      # Forward pass\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "  # Evaluate on the test set\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZRl3xrHtrEN",
        "outputId": "6e30f329-6593-420b-ad0c-f8e5bfce634d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "************************* \n",
            "\n",
            "*** Strategy: ZERO ***\n",
            "\n",
            "* Before redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[-0.1416, -0.0199],\n",
            "        [ 0.0670, -0.0019]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  tensor(2.2325, grad_fn=<SumBackward0>)\n",
            "\n",
            "\n",
            "* After redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[0., 0.],\n",
            "        [0., 0.]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  tensor(0., grad_fn=<SumBackward0>)\n",
            "\n",
            "\n",
            "*** Training \n",
            "\n",
            "Epoch [1/10], Loss: 2.3021\n",
            "Epoch [2/10], Loss: 2.3059\n",
            "Epoch [3/10], Loss: 2.3018\n",
            "Epoch [4/10], Loss: 2.2996\n",
            "Epoch [5/10], Loss: 2.3065\n",
            "Epoch [6/10], Loss: 2.3032\n",
            "Epoch [7/10], Loss: 2.3006\n",
            "Epoch [8/10], Loss: 2.3030\n",
            "Epoch [9/10], Loss: 2.3022\n",
            "Epoch [10/10], Loss: 2.3032\n",
            "Test Accuracy: 10.00%\n",
            "\n",
            "\n",
            "************************* \n",
            "\n",
            "*** Strategy: RANDOM ***\n",
            "\n",
            "* Before redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[0., 0.],\n",
            "        [0., 0.]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  tensor(0., grad_fn=<SumBackward0>)\n",
            "\n",
            "\n",
            "* After redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[ 0.2839, -0.3263],\n",
            "        [ 0.8794,  0.2129]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  tensor(29.7830, grad_fn=<SumBackward0>)\n",
            "\n",
            "\n",
            "*** Training \n",
            "\n",
            "Epoch [1/10], Loss: 690.2726\n",
            "Epoch [2/10], Loss: 444.6983\n",
            "Epoch [3/10], Loss: 242.8028\n",
            "Epoch [4/10], Loss: 101.7409\n",
            "Epoch [5/10], Loss: 67.7767\n",
            "Epoch [6/10], Loss: 10.4618\n",
            "Epoch [7/10], Loss: 2.2267\n",
            "Epoch [8/10], Loss: 2.5129\n",
            "Epoch [9/10], Loss: 2.2143\n",
            "Epoch [10/10], Loss: 2.1288\n",
            "Test Accuracy: 11.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full solution code"
      ],
      "metadata": {
        "id": "g2rBrSMG1kBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "results = {}\n",
        "\n",
        "# prepare init strategies\n",
        "# by defaule in PyTorch nn.Conv2d ‚Üí kaiming_uniform, nn.Linear ‚Üí kaiming_uniform_ (ReLU) or xavier_uniform_\n",
        "init_strategies = ['default', 'zero', 'random', 'xavier', 'kaiming']\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the CNN Model\n",
        "# TODO: Add convolutional, pooling, and fully connected layers\n",
        "class CNNModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "\n",
        "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(32 * 8 * 8, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 10)\n",
        "  )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.network(x)\n",
        "\n",
        "# apply(fn) ‚Äî recursively applies the function fn to all submodules (including self), calling fn(module) for each.\n",
        "# I initialize bias to 0 because this is standard practice: weights start out distributed around 0, and bias should be symmetrical.\n",
        "def make_initializer(strategy):\n",
        "  def init_fn(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "      if strategy == 'default':\n",
        "        pass\n",
        "      if strategy == 'zero':\n",
        "        nn.init.zeros_(m.weight)\n",
        "        if m.bias is not None:\n",
        "          nn.init.zeros_(m.bias)\n",
        "      elif strategy == 'random':\n",
        "        nn.init.normal_(m.weight, 0.0, 1.0)\n",
        "        if m.bias is not None:\n",
        "          nn.init.zeros_(m.bias)\n",
        "      elif strategy == 'xavier':\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "          nn.init.zeros_(m.bias)\n",
        "      elif strategy == 'kaiming':\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "          nn.init.zeros_(m.bias)\n",
        "  return init_fn\n",
        "\n",
        "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs=10):\n",
        "  loss_history = []\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    for images, labels in train_loader:\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_losses.append(loss.item())\n",
        "\n",
        "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    loss_history.append(avg_loss)\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "  # Evaluate\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  return loss_history, accuracy\n",
        "\n",
        "for init_strategy in init_strategies:\n",
        "  print(\"\\n\" + \"=\"*40)\n",
        "  print(f\"*** Strategy: {init_strategy.upper()} ***\")\n",
        "  print(\"=\"*40)\n",
        "\n",
        "  model = CNNModel()  # ‚Üê –ù–û–í–ê–Ø –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏!\n",
        "\n",
        "  print(\"* Before redefining * \\n\")\n",
        "  print(\"Part of weights: \\n\\n\", model.network[0].weight[0, 0, :2, :2], \"\\n\")\n",
        "  print(\"Sum of weights: \", model.network[0].weight.sum().item())\n",
        "  print(\"\\n\")\n",
        "\n",
        "  model.apply(make_initializer(init_strategy))\n",
        "\n",
        "  print(\"* After redefining * \\n\")\n",
        "  print(\"Part of weights: \\n\\n\", model.network[0].weight[0, 0, :2, :2], \"\\n\")\n",
        "  print(\"Sum of weights: \", model.network[0].weight.sum().item())\n",
        "  print(\"\\n\")\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  loss_history, accuracy = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs=8)\n",
        "\n",
        "  results[init_strategy] = {\n",
        "      'loss_history': loss_history,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "\n",
        "  print(f\"\\nFinal Test Accuracy: {accuracy:.2f}%\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Summary of Initialization Strategies\")\n",
        "print(\"=\"*50)\n",
        "print(f\"{'Strategy':<10} | {'Final Loss':<12} | {'Accuracy':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for strategy, data in results.items():\n",
        "    final_loss = data['loss_history'][-1]\n",
        "    accuracy = data['accuracy']\n",
        "    print(f\"{strategy:<10} | {final_loss:<12.4f} | {accuracy:<9.2f}%\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_jk3RmUwg-X",
        "outputId": "47320b52-e602-441f-d05d-ab455b6ea097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "*** Strategy: DEFAULT ***\n",
            "========================================\n",
            "* Before redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[ 0.1077, -0.1501],\n",
            "        [-0.0384,  0.0809]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  -1.3259371519088745\n",
            "\n",
            "\n",
            "* After redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[ 0.1077, -0.1501],\n",
            "        [-0.0384,  0.0809]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  -1.3259371519088745\n",
            "\n",
            "\n",
            "Epoch [1/8], Loss: 1.3928\n",
            "Epoch [2/8], Loss: 1.0373\n",
            "Epoch [3/8], Loss: 0.8621\n",
            "Epoch [4/8], Loss: 0.7250\n",
            "Epoch [5/8], Loss: 0.6030\n",
            "Epoch [6/8], Loss: 0.4871\n",
            "Epoch [7/8], Loss: 0.3760\n",
            "Epoch [8/8], Loss: 0.2770\n",
            "\n",
            "Final Test Accuracy: 71.13%\n",
            "\n",
            "\n",
            "========================================\n",
            "*** Strategy: ZERO ***\n",
            "========================================\n",
            "* Before redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[-0.1660, -0.0113],\n",
            "        [ 0.0037, -0.0995]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  0.19332683086395264\n",
            "\n",
            "\n",
            "* After redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[0., 0.],\n",
            "        [0., 0.]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  0.0\n",
            "\n",
            "\n",
            "Epoch [1/8], Loss: 2.3027\n",
            "Epoch [2/8], Loss: 2.3027\n",
            "Epoch [3/8], Loss: 2.3027\n",
            "Epoch [4/8], Loss: 2.3027\n",
            "Epoch [5/8], Loss: 2.3027\n",
            "Epoch [6/8], Loss: 2.3027\n",
            "Epoch [7/8], Loss: 2.3027\n",
            "Epoch [8/8], Loss: 2.3027\n",
            "\n",
            "Final Test Accuracy: 10.00%\n",
            "\n",
            "\n",
            "========================================\n",
            "*** Strategy: RANDOM ***\n",
            "========================================\n",
            "* Before redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[-0.0015,  0.0960],\n",
            "        [ 0.1394, -0.0402]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  -1.7274479866027832\n",
            "\n",
            "\n",
            "* After redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[ 0.0947,  0.5082],\n",
            "        [-0.4299,  0.9711]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  -33.129661560058594\n",
            "\n",
            "\n",
            "Epoch [1/8], Loss: 3721.9525\n",
            "Epoch [2/8], Loss: 1113.4085\n",
            "Epoch [3/8], Loss: 577.1094\n",
            "Epoch [4/8], Loss: 336.7363\n",
            "Epoch [5/8], Loss: 207.8426\n",
            "Epoch [6/8], Loss: 133.5023\n",
            "Epoch [7/8], Loss: 87.3257\n",
            "Epoch [8/8], Loss: 57.3428\n",
            "\n",
            "Final Test Accuracy: 28.92%\n",
            "\n",
            "\n",
            "========================================\n",
            "*** Strategy: XAVIER ***\n",
            "========================================\n",
            "* Before redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[ 0.1310, -0.0541],\n",
            "        [ 0.1556,  0.1181]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  0.4815179109573364\n",
            "\n",
            "\n",
            "* After redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[0.0447, 0.0283],\n",
            "        [0.0462, 0.2697]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  0.2154557704925537\n",
            "\n",
            "\n",
            "Epoch [1/8], Loss: 1.3297\n",
            "Epoch [2/8], Loss: 0.9681\n",
            "Epoch [3/8], Loss: 0.7996\n",
            "Epoch [4/8], Loss: 0.6586\n",
            "Epoch [5/8], Loss: 0.5225\n",
            "Epoch [6/8], Loss: 0.3973\n",
            "Epoch [7/8], Loss: 0.2764\n",
            "Epoch [8/8], Loss: 0.1859\n",
            "\n",
            "Final Test Accuracy: 70.12%\n",
            "\n",
            "\n",
            "========================================\n",
            "*** Strategy: KAIMING ***\n",
            "========================================\n",
            "* Before redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[-0.0344,  0.0164],\n",
            "        [-0.1491,  0.0300]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  -1.9606029987335205\n",
            "\n",
            "\n",
            "* After redefining * \n",
            "\n",
            "Part of weights: \n",
            "\n",
            " tensor([[-0.1312, -0.4163],\n",
            "        [ 0.4731,  0.2146]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Sum of weights:  -0.10756230354309082\n",
            "\n",
            "\n",
            "Epoch [1/8], Loss: 1.3512\n",
            "Epoch [2/8], Loss: 0.9844\n",
            "Epoch [3/8], Loss: 0.8111\n",
            "Epoch [4/8], Loss: 0.6697\n",
            "Epoch [5/8], Loss: 0.5370\n",
            "Epoch [6/8], Loss: 0.4027\n",
            "Epoch [7/8], Loss: 0.2891\n",
            "Epoch [8/8], Loss: 0.1984\n",
            "\n",
            "Final Test Accuracy: 68.98%\n",
            "\n",
            "\n",
            "==================================================\n",
            "Summary of Initialization Strategies\n",
            "==================================================\n",
            "Strategy   | Final Loss   | Accuracy  \n",
            "--------------------------------------------------\n",
            "default    | 0.2770       | 71.13    %\n",
            "zero       | 2.3027       | 10.00    %\n",
            "random     | 57.3428      | 28.92    %\n",
            "xavier     | 0.1859       | 70.12    %\n",
            "kaiming    | 0.1984       | 68.98    %\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "| Strategy | Final Loss | Accuracy | Status |\n",
        "|----------|------------|----------|--------|\n",
        "| default  | 0.2770     | 71.13%   | ‚úÖ **Best** |\n",
        "| zero     | 2.3027     | 10.00%   | üî¥ **Failed** |\n",
        "| random   | 57.3428    | 28.92%   | üü° **Poor** |\n",
        "| xavier   | 0.1859     | 70.12%   | üü¢ **Good** |\n",
        "| kaiming  | 0.1984     | 68.98%   | üü¢ **Good** |\n",
        "\n",
        "---\n",
        "\n",
        "- Zero initialization is disastrous (accuracy = 10% - random guessing)\n",
        "- Random without scaling is also bad (huge losses)\n",
        "- Xavier and Kaiming perform almost as well as default\n",
        "- PyTorch's default shows the best results"
      ],
      "metadata": {
        "id": "aQV9A8as4AYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why initialization is important in practice, thoughts\n",
        "\n",
        "Until 2015, deep networks (more than 20-30 layers) had issues with stable training. They suffered from two problems:\n",
        "- Vanishing gradients ‚Äì gradients became too small\n",
        "- Exploding gradients ‚Äì gradients became too large\n",
        "\n",
        "In 2015, Kaiming He proposed an initialization specifically designed for ReLU activations and then (together with the Microsoft Research team) presented the 152-layer ResNet convolutional network:\n",
        "- Top-5 error: 3.57% (exceeded human performance for the first time!)\n",
        "- Won the ImageNet 2015 competition\n",
        "\n",
        "Key innovations of ResNet:\n",
        "- Skip connections and working with ReLUs and BatchNorm\n",
        "- Residual learning - learns the difference (residual) rather than a direct transformation\n",
        "\n",
        "Standard initialization (Xavier/Glorot) worked well from 2010 for sigmoid/tanh, but failed for ReLU.\n",
        "\n",
        "Kaiming and Xavier are still the basis of production training. It is Important to remember that in modern architectures (e.g. Transformers) these initialization schemes are often modified and adapted to specific layers (e.g. the initialization of weights in linear layers and attention layers may differ).\n",
        "\n",
        "### Fine-tuning\n",
        "Don't reinitialize the embedding layers‚Äîthey've already been trained!\n",
        "But for new head layers, Kaiming/Xavier are a must.\n",
        "\n",
        "## Post-Thinking notes\n",
        "\n",
        "Even with Kaiming/Xavier initialization and BatchNorm, training ultra-deep networks (hundreds or thousands of layers) remains challenging.\n",
        "BatchNorm itself introduces limitations: it depends on batch size, breaks in small-batch regimes (e.g., RL, medical imaging), and is incompatible with recurrent architectures.\n",
        "\n",
        "Modern approaches - such as Fixup, ReZero, and DeepNorm - shift the paradigm:\n",
        "instead of preserving variance layer by layer (as Xavier/Kaiming do), they are designed to ensure stable signal propagation in the infinite-depth limit..\n",
        "\n",
        "For example, they initialise residual branches to zero or scale them by `1/‚àöL`, making the network behave like an identity map at initialization - which enables training of 1000+ layer models even without normalization layers\n",
        "\n",
        "This group of methods is the next evolutionary step after He/Xavier for extreme scenarios."
      ],
      "metadata": {
        "id": "cJiwhJULEayX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interesting Publications\n",
        "\n",
        "- The task, without a solution, was taken from [here](https://github.com/Exorust/TorchLeet/tree/main/torch/medium)\n",
        "\n",
        "### Kaiming He, ResNet\n",
        "- [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. (Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun)](https://arxiv.org/abs/1502.01852)\n",
        "- [Deep Residual Learning for Image Recognition. (Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun)](https://arxiv.org/abs/1512.03385)\n",
        "- [Identity Mappings in Deep Residual Networks. (Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun)](https://arxiv.org/abs/1603.05027)\n",
        "\n",
        "### Useful niche solutions\n",
        "- Fixup (Training without BatchNorm, batch size < 8) -> RL without normalization\n",
        "  - [Fixup Initialization: Residual Learning Without Normalization. (Hongyi Zhang, Yann N. Dauphin, Tengyu Ma)](https://arxiv.org/abs/1901.09321)\n",
        "- T-Fixup (Transformers without LayerNorm) -> Transformers on edge devices\n",
        "- LayerScale (Weak residual links in ViTs) -> ViT research"
      ],
      "metadata": {
        "id": "k3fMyY5EFPhK"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4L12POpZPL2K",
        "OGkG2nXf1er9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}