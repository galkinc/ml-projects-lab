{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPKFpV3jQ01gdSgQ9HkS/6m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Build the Neural Network\n","\n","[Link to the related offical tutorial](https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html). Based by Update:: Jan 24, 2025\n","\n","Neural networks comprise of layers/modules that perform operations on data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n","\n","In the following sections, we’ll build a neural network to classify images in the FashionMNIST dataset."],"metadata":{"id":"OXIkbf4FFuN9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3dFDtHayFlyd"},"outputs":[],"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"]},{"cell_type":"markdown","source":["# Get Device for Training\n","\n","We want to be able to train our model on an [accelerator](https://pytorch.org/docs/stable/torch.html#accelerators) such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU."],"metadata":{"id":"9ok4DAEWH_eG"}},{"cell_type":"code","source":["device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7AljespH7HC","executionInfo":{"status":"ok","timestamp":1761142756139,"user_tz":-180,"elapsed":15,"user":{"displayName":"Cyril Galkin","userId":"06561683218058714985"}},"outputId":"9d491bf1-dcba-47f7-fec6-1efe6b66ead2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n"]}]},{"cell_type":"markdown","source":["# Define the Class\n","\n","We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method."],"metadata":{"id":"lFzvOv87Sfwb"}},{"cell_type":"code","source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"],"metadata":{"id":"d3cimrleSpKh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We create an instance of `NeuralNetwork`, and move it to the `device`, and print its structure."],"metadata":{"id":"Or_ja172SuS9"}},{"cell_type":"code","source":["model = NeuralNetwork().to(device)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkULFuwISyUY","executionInfo":{"status":"ok","timestamp":1761142844840,"user_tz":-180,"elapsed":42,"user":{"displayName":"Cyril Galkin","userId":"06561683218058714985"}},"outputId":"b8f90f60-8085-46fc-8c67-adfaf42f99a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["To use the model, we pass it the input data. This executes the model’s `forward`, along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). Do not call `model.forward()` directly!\n","\n","Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the `nn.Softmax` module."],"metadata":{"id":"izWbQPMzTa-B"}},{"cell_type":"code","source":["X = torch.rand(1, 28, 28, device=device)\n","logits = model(X)\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(f\"Predicted class: {y_pred}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CmjIdae7T0Ys","executionInfo":{"status":"ok","timestamp":1761143137016,"user_tz":-180,"elapsed":15,"user":{"displayName":"Cyril Galkin","userId":"06561683218058714985"}},"outputId":"56eb2a45-f223-4bd6-9557-dc0f6f04b01b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: tensor([7])\n"]}]},{"cell_type":"markdown","source":["# Model Layers\n","\n","Let’s break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network."],"metadata":{"id":"Cj-5MVOJT7zo"}},{"cell_type":"code","source":["input_image = torch.rand(3,28,28)\n","print(input_image.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nncV5n-YUAjz","executionInfo":{"status":"ok","timestamp":1761143164618,"user_tz":-180,"elapsed":18,"user":{"displayName":"Cyril Galkin","userId":"06561683218058714985"}},"outputId":"d27c520f-5aee-443c-be80-64f6ce5423a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 28, 28])\n"]}]},{"cell_type":"markdown","source":["# nn.Flatten\n","\n","We initialize the nn.Flatten layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained)."],"metadata":{"id":"iSsqUTa_UCrQ"}},{"cell_type":"code","source":["flatten = nn.Flatten()\n","flat_image = flatten(input_image)\n","print(flat_image.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpypKxKnUICS","executionInfo":{"status":"ok","timestamp":1761143196591,"user_tz":-180,"elapsed":8,"user":{"displayName":"Cyril Galkin","userId":"06561683218058714985"}},"outputId":"68f04841-6a4e-46c6-8380-485ae84b094d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 784])\n"]}]},{"cell_type":"markdown","source":["# nn.Linear\n","\n","The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using its stored weights and biases."],"metadata":{"id":"pYRx-Uv2UKWJ"}},{"cell_type":"code","source":["layer1 = nn.Linear(in_features=28*28, out_features=20)\n","hidden1 = layer1(flat_image)\n","print(hidden1.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dGMDjjZMURY4","executionInfo":{"status":"ok","timestamp":1761143234562,"user_tz":-180,"elapsed":18,"user":{"displayName":"Cyril Galkin","userId":"06561683218058714985"}},"outputId":"c96e2b8d-22f2-4cde-9d7d-211941c40d8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 20])\n"]}]},{"cell_type":"markdown","source":["# nn.ReLU\n","\n","Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n","\n","In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there’s other activations to introduce non-linearity in your model."],"metadata":{"id":"jPgD0cr0UTlN"}},{"cell_type":"code","source":["print(f\"Before ReLU: {hidden1}\\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU: {hidden1}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dd0sbqciUb0W","executionInfo":{"status":"ok","timestamp":1761143276355,"user_tz":-180,"elapsed":26,"user":{"displayName":"Cyril Galkin","userId":"06561683218058714985"}},"outputId":"6c7b54e0-bf36-407d-dd03-0551f5aa5b5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before ReLU: tensor([[-0.1246, -0.5447, -0.2723, -0.2683,  0.0014, -0.1697, -0.3985,  0.5260,\n","          0.1221, -0.2756,  0.4873, -0.1769, -0.3930,  0.1267,  0.1994,  0.6850,\n","         -0.2002,  0.2494, -0.1476,  0.2757],\n","        [ 0.0096, -0.7805, -0.5840, -0.1280, -0.1715, -0.3006, -0.3051,  0.1480,\n","          0.2628, -0.7782,  0.0076, -0.0133, -0.5557,  0.2303,  0.5479,  0.0602,\n","          0.0637,  0.5342, -0.3298,  0.3535],\n","        [ 0.2356, -0.6195, -0.0212, -0.3030,  0.2449, -0.2861, -0.1756,  0.3741,\n","          0.3835, -0.3160,  0.2109,  0.0772, -0.4899,  0.1329,  0.2629,  0.0352,\n","         -0.1810,  0.2937, -0.1387,  0.4474]], grad_fn=<AddmmBackward0>)\n","\n","\n","After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0014, 0.0000, 0.0000, 0.5260, 0.1221,\n","         0.0000, 0.4873, 0.0000, 0.0000, 0.1267, 0.1994, 0.6850, 0.0000, 0.2494,\n","         0.0000, 0.2757],\n","        [0.0096, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1480, 0.2628,\n","         0.0000, 0.0076, 0.0000, 0.0000, 0.2303, 0.5479, 0.0602, 0.0637, 0.5342,\n","         0.0000, 0.3535],\n","        [0.2356, 0.0000, 0.0000, 0.0000, 0.2449, 0.0000, 0.0000, 0.3741, 0.3835,\n","         0.0000, 0.2109, 0.0772, 0.0000, 0.1329, 0.2629, 0.0352, 0.0000, 0.2937,\n","         0.0000, 0.4474]], grad_fn=<ReluBackward0>)\n"]}]},{"cell_type":"markdown","source":["# nn.Sequential\n","\n","[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like `seq_modules`."],"metadata":{"id":"NwEC0UaoUeWR"}},{"cell_type":"code","source":["seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","input_image = torch.rand(3,28,28)\n","logits = seq_modules(input_image)\n","\n","input_image, logits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFzJw88JUkYm","executionInfo":{"status":"ok","timestamp":1761143333641,"user_tz":-180,"elapsed":21,"user":{"displayName":"Cyril Galkin","userId":"06561683218058714985"}},"outputId":"79e46e54-7d59-45be-8c85-eb2dd6312488"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[0.7678, 0.3577, 0.7353,  ..., 0.0423, 0.5738, 0.4122],\n","          [0.4902, 0.3629, 0.5518,  ..., 0.2843, 0.7921, 0.9271],\n","          [0.3213, 0.4581, 0.1752,  ..., 0.1405, 0.9648, 0.6494],\n","          ...,\n","          [0.7226, 0.0259, 0.0072,  ..., 0.9213, 0.1015, 0.0699],\n","          [0.9119, 0.2159, 0.2805,  ..., 0.3655, 0.0987, 0.5805],\n","          [0.4949, 0.6785, 0.9528,  ..., 0.8427, 0.0153, 0.5446]],\n"," \n","         [[0.0855, 0.7375, 0.9145,  ..., 0.5742, 0.3839, 0.1516],\n","          [0.9692, 0.9947, 0.6562,  ..., 0.6730, 0.9904, 0.7877],\n","          [0.2156, 0.1273, 0.4750,  ..., 0.6029, 0.1687, 0.3495],\n","          ...,\n","          [0.1844, 0.0099, 0.9179,  ..., 0.5508, 0.7832, 0.2717],\n","          [0.8393, 0.3042, 0.7636,  ..., 0.2134, 0.8421, 0.4424],\n","          [0.8583, 0.6771, 0.9145,  ..., 0.0959, 0.6862, 0.5294]],\n"," \n","         [[0.1219, 0.4887, 0.4164,  ..., 0.4732, 0.5373, 0.4662],\n","          [0.8333, 0.3444, 0.0970,  ..., 0.8987, 0.1803, 0.5006],\n","          [0.8669, 0.2837, 0.6210,  ..., 0.0089, 0.8391, 0.7552],\n","          ...,\n","          [0.8449, 0.4321, 0.9643,  ..., 0.0682, 0.9101, 0.9279],\n","          [0.0229, 0.2385, 0.1413,  ..., 0.5891, 0.4639, 0.7381],\n","          [0.9494, 0.2055, 0.2750,  ..., 0.9119, 0.6860, 0.2010]]]),\n"," tensor([[-7.4952e-02, -1.7149e-02,  1.5239e-04,  7.9780e-02,  1.0224e-01,\n","          -1.5156e-01,  1.3642e-01,  2.4994e-01, -4.7631e-03, -1.1721e-01],\n","         [-6.9277e-02, -1.6432e-02,  7.5870e-02,  8.7509e-02,  1.4022e-01,\n","          -1.5173e-01,  1.5974e-01,  3.5456e-01,  1.2803e-01, -8.2389e-02],\n","         [-6.6188e-02, -1.2453e-02,  2.9732e-02,  5.4459e-02,  8.1163e-02,\n","          -1.7887e-01,  1.8794e-01,  2.3081e-01,  1.2128e-02, -4.0240e-02]],\n","        grad_fn=<AddmmBackward0>))"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["# nn.Softmax\n","\n","The last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the nn.Softmax module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1."],"metadata":{"id":"JfliV3vyUwK9"}},{"cell_type":"code","source":["softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)\n","\n","pred_probab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0Kel4EAU3fF","executionInfo":{"status":"ok","timestamp":1761143394767,"user_tz":-180,"elapsed":51,"user":{"displayName":"Cyril Galkin","userId":"06561683218058714985"}},"outputId":"a4093516-30dd-4cbd-eece-8dfb632b46b1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0903, 0.0957, 0.0973, 0.1054, 0.1078, 0.0836, 0.1115, 0.1249, 0.0969,\n","         0.0866],\n","        [0.0868, 0.0915, 0.1003, 0.1015, 0.1070, 0.0799, 0.1091, 0.1326, 0.1057,\n","         0.0856],\n","        [0.0903, 0.0952, 0.0993, 0.1018, 0.1046, 0.0806, 0.1164, 0.1215, 0.0976,\n","         0.0926]], grad_fn=<SoftmaxBackward0>)"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["# Model Parameters\n","\n","Many layers inside a neural network are *parameterized*, i.e. have associated weights and biases that are optimized during training. Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s `parameters()` or `named_parameters()` methods.\n","\n","In this example, we iterate over each parameter, and print its size and a preview of its values."],"metadata":{"id":"BypkoNsSU8Ll"}},{"cell_type":"code","source":["print(f\"Model structure: {model}\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vfQ2cIYVKLX","executionInfo":{"status":"ok","timestamp":1761143467110,"user_tz":-180,"elapsed":17,"user":{"displayName":"Cyril Galkin","userId":"06561683218058714985"}},"outputId":"8881c38d-e23b-434f-8e6e-b3a608e00b12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure: NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n","\n","\n","Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0062, -0.0178, -0.0327,  ...,  0.0063, -0.0132,  0.0128],\n","        [-0.0128,  0.0200, -0.0029,  ...,  0.0302, -0.0164, -0.0269]],\n","       grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0057, -0.0257], grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0190,  0.0243, -0.0012,  ..., -0.0224, -0.0251,  0.0277],\n","        [ 0.0355,  0.0387,  0.0048,  ..., -0.0108, -0.0022,  0.0171]],\n","       grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0055, -0.0297], grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0224, -0.0162, -0.0109,  ..., -0.0005,  0.0275, -0.0112],\n","        [-0.0146, -0.0245, -0.0094,  ..., -0.0372,  0.0394, -0.0309]],\n","       grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0045, 0.0331], grad_fn=<SliceBackward0>) \n","\n"]}]},{"cell_type":"markdown","source":["# Further Reading\n","\n","- [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n","\n"],"metadata":{"id":"1iRpjHrA0qJj"}}]}