{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsHV-7cpVkyK"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "atWM-s8yVnfX",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:20:13.584667Z",
     "end_time": "2023-05-08T16:20:13.587169Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB0wBWfcVqHz"
   },
   "source": [
    "# Hyperparameter Tuning with the HParams Dashboard\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/hyperparameter_tuning_with_hparams.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorboard/blob/master/docs/hyperparameter_tuning_with_hparams.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorboard/docs/hyperparameter_tuning_with_hparams.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elH58gbhWAmn"
   },
   "source": [
    "When building machine learning models, you need to choose various [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)), such as the dropout rate in a layer or the learning rate. These decisions impact model metrics, such as accuracy. Therefore, an important step in the machine learning workflow is to identify the best hyperparameters for your problem, which often involves experimentation. This process is known as \"Hyperparameter Optimization\" or \"Hyperparameter Tuning\".\n",
    "\n",
    "The HParams dashboard in TensorBoard provides several tools to help with this process of identifying the best experiment or most promising sets of hyperparameters. \n",
    "\n",
    "This tutorial will focus on the following steps:\n",
    "\n",
    "1. Experiment setup and HParams summary\n",
    "2. Adapt TensorFlow runs to log hyperparameters and metrics\n",
    "3. Start runs and log them all under one parent directory\n",
    "4. Visualize the results in TensorBoard's HParams dashboard\n",
    "\n",
    "Note: The HParams summary APIs and dashboard UI are in a preview stage and will change over time. \n",
    "\n",
    "Start by installing TF 2.0 and loading the TensorBoard notebook extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8p3Tbx8cWEFA",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:20:13.593168Z",
     "end_time": "2023-05-08T16:20:13.604580Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lEWCCQYkWIdA",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:20:13.604580Z",
     "end_time": "2023-05-08T16:20:14.192826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GtR_cTTkf9G"
   },
   "source": [
    "Import TensorFlow and the TensorBoard HParams plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mVtYvbbIWRkV",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:20:14.196830Z",
     "end_time": "2023-05-08T16:20:15.878770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 17:20:14.416341: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 17:20:15.090145: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfCa27_8kov6"
   },
   "source": [
    "Download the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset and scale it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z8b82G7YksOS",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:20:15.883301Z",
     "end_time": "2023-05-08T16:20:16.209977Z"
    }
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tsTxO85WaYq"
   },
   "source": [
    "## 1. Experiment setup and the HParams experiment summary\n",
    "\n",
    "Experiment with three hyperparameters in the model:\n",
    "\n",
    "1. Number of units in the first dense layer\n",
    "2. Dropout rate in the dropout layer\n",
    "3. Optimizer\n",
    "\n",
    "List the values to try, and log an experiment configuration to TensorBoard. This step is optional: you can provide domain information to enable more precise filtering of hyperparameters in the UI, and you can specify which metrics should be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5Euw0agpWb4V",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:20:16.213978Z",
     "end_time": "2023-05-08T16:20:16.971166Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 17:20:16.233495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-08 17:20:16.263450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-08 17:20:16.263515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-08 17:20:16.264737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-08 17:20:16.264791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-08 17:20:16.264819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-08 17:20:16.928047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-08 17:20:16.928309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-08 17:20:16.928324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-05-08 17:20:16.928375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-08 17:20:16.928404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5897 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32]))\n",
    "#HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    #hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],\n",
    "    hparams=[HP_NUM_UNITS, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_T95RrSIVO6"
   },
   "source": [
    "If you choose to skip this step, you can use a string literal wherever you would otherwise use an `HParam` value: e.g., `hparams['dropout']` instead of `hparams[HP_DROPOUT]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "va9XMh-dW4_f"
   },
   "source": [
    "## 2. Adapt TensorFlow runs to log hyperparameters and metrics\n",
    "\n",
    "The model will be quite simple: two dense layers with a dropout layer between them. The training code will look familiar, although the hyperparameters are no longer hardcoded. Instead, the hyperparameters are provided in an `hparams` dictionary and used throughout the training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hG-zalNfW5Zl",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:20:16.971166Z",
     "end_time": "2023-05-08T16:20:16.973667Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "  model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),\n",
    "    #tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "  ])\n",
    "  model.compile(\n",
    "      optimizer=hparams[HP_OPTIMIZER],\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['accuracy'],\n",
    "  )\n",
    "\n",
    "  model.fit(x_train, y_train, epochs=1) # Run with 1 epoch to speed things up for demo purposes\n",
    "  _, accuracy = model.evaluate(x_test, y_test)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46oJF8seXM7v"
   },
   "source": [
    "For each run, log an hparams summary with the hyperparameters and final accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8j-fO6nEXRfW",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:20:16.977167Z",
     "end_time": "2023-05-08T16:20:16.983665Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mYdW0VKLbFE"
   },
   "source": [
    "When training Keras models, you can use callbacks instead of writing these directly:\n",
    "\n",
    "```python\n",
    "model.fit(\n",
    "    ...,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.TensorBoard(logdir),  # log metrics\n",
    "        hp.KerasCallback(logdir, hparams),  # log hparams\n",
    "    ],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2nOgIKAXdcb"
   },
   "source": [
    "## 3. Start runs and log them all under one parent directory\n",
    "\n",
    "You can now try multiple experiments, training each one with a different set of hyperparameters. \n",
    "\n",
    "For simplicity, use a grid search: try all combinations of the discrete parameters and just the lower and upper bounds of the real-valued parameter. For more complex scenarios, it might be more effective to choose each hyperparameter value randomly (this is called a random search). There are more advanced methods that can be used.\n",
    "\n",
    "Run a few experiments, which will take a few minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KbqT5n-AXd0h",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:20:16.983665Z",
     "end_time": "2023-05-08T16:20:49.267879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units': 16, 'optimizer': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 17:20:18.529906: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fa8644f71a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-08 17:20:18.529948: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 2070 with Max-Q Design, Compute Capability 7.5\n",
      "2023-05-08 17:20:18.533595: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-08 17:20:19.725377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-08 17:20:19.831047: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-08 17:20:19.899965: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 8s 3ms/step - loss: 0.7222 - accuracy: 0.7510\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5065 - accuracy: 0.8216\n",
      "--- Starting trial: run-1\n",
      "{'num_units': 16, 'optimizer': 'sgd'}\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.9828 - accuracy: 0.6590\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.7754\n",
      "--- Starting trial: run-2\n",
      "{'num_units': 32, 'optimizer': 'adam'}\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6156 - accuracy: 0.7843\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4764 - accuracy: 0.8336\n",
      "--- Starting trial: run-3\n",
      "{'num_units': 32, 'optimizer': 'sgd'}\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8916 - accuracy: 0.6969\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.7880\n",
      "CPU times: user 29.4 s, sys: 9.57 s, total: 39 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "  #for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "  for optimizer in HP_OPTIMIZER.domain.values:\n",
    "    hparams = {\n",
    "        HP_NUM_UNITS: num_units,\n",
    "        #HP_DROPOUT: dropout_rate,\n",
    "        HP_OPTIMIZER: optimizer,\n",
    "    }\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    run('logs/hparam_tuning/' + run_name, hparams)\n",
    "    session_num += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSyjWQ3mPKT9"
   },
   "source": [
    "## 4. Visualize the results in TensorBoard's HParams plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VBvplwyP8Vy"
   },
   "source": [
    "The HParams dashboard can now be opened. Start TensorBoard and click on \"HParams\" at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Xf4KM-U2bbP_",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:21:19.643689Z",
     "end_time": "2023-05-08T16:21:19.654187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-fda7792c3c9760f0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-fda7792c3c9760f0\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTWg9nXnxWWI"
   },
   "source": [
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/hparams_table.png?raw=1\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RPGbR9EWd4o"
   },
   "source": [
    "The left pane of the dashboard provides filtering capabilities that are active across all the views in the HParams dashboard:\n",
    "\n",
    "- Filter which hyperparameters/metrics are shown in the dashboard\n",
    "- Filter which hyperparameter/metrics values are shown in the dashboard\n",
    "- Filter on run status (running, success, ...)\n",
    "- Sort by hyperparameter/metric in the table view\n",
    "- Number of session groups to show (useful for performance when there are many experiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3Q5v28XaCt8"
   },
   "source": [
    "The HParams dashboard has three different views, with various useful information:\n",
    "\n",
    "* The **Table View** lists the runs, their hyperparameters, and their metrics.\n",
    "* The **Parallel Coordinates View** shows each run as a line going through an axis for each hyperparemeter and metric. Click and drag the mouse on any axis to mark a region which will highlight only the runs that pass through it. This can be useful for identifying which groups of hyperparameters are most important. The axes themselves can be re-ordered by dragging them.\n",
    "* The **Scatter Plot View** shows plots comparing each hyperparameter/metric with each metric. This can help identify correlations. Click and drag to select a region in a specific plot and highlight those sessions across the other plots. \n",
    "\n",
    "A table row, a parallel coordinates line, and a scatter plot market can be clicked to see a plot of the metrics as a function of training steps for that session (although in this tutorial only one step is used for each run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fh3p0DtcdOK1"
   },
   "source": [
    "To further explore the capabilities of the HParams dashboard, download a set of pregenerated logs with more experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oxrSUAnCeFmQ",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:20:52.818271Z",
     "end_time": "2023-05-08T16:20:58.862694Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: /home/galkinc/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "replace logs/hparam_demo/hparams_demo/0/events.out.tfevents.1550612933.goshri.c.googlers.com? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b\"wget -q 'https://storage.googleapis.com/download.tensorflow.org/tensorboard/hparams_demo_logs.zip'\\nunzip -q hparams_demo_logs.zip -d logs/hparam_demo\\n\"' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_cell_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbash\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwget -q \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhttps://storage.googleapis.com/download.tensorflow.org/tensorboard/hparams_demo_logs.zip\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43munzip -q hparams_demo_logs.zip -d logs/hparam_demo\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2475\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2473\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2474\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2475\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2477\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2478\u001B[0m \u001B[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2479\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2480\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/magics/script.py:153\u001B[0m, in \u001B[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001B[0;34m(line, cell)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    152\u001B[0m     line \u001B[38;5;241m=\u001B[39m script\n\u001B[0;32m--> 153\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshebang\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/magics/script.py:305\u001B[0m, in \u001B[0;36mScriptMagics.shebang\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mraise_error \u001B[38;5;129;01mand\u001B[39;00m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001B[39;00m\n\u001B[1;32m    302\u001B[0m     \u001B[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001B[39;00m\n\u001B[1;32m    303\u001B[0m     \u001B[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001B[39;00m\n\u001B[1;32m    304\u001B[0m     rc \u001B[38;5;241m=\u001B[39m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m9\u001B[39m\n\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CalledProcessError(rc, cell)\n",
      "\u001B[0;31mCalledProcessError\u001B[0m: Command 'b\"wget -q 'https://storage.googleapis.com/download.tensorflow.org/tensorboard/hparams_demo_logs.zip'\\nunzip -q hparams_demo_logs.zip -d logs/hparam_demo\\n\"' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget -q 'https://storage.googleapis.com/download.tensorflow.org/tensorboard/hparams_demo_logs.zip'\n",
    "unzip -q hparams_demo_logs.zip -d logs/hparam_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__8xQhjqgr3D"
   },
   "source": [
    "View these logs in TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "KBHp6M_zgjp4",
    "ExecuteTime": {
     "start_time": "2023-05-08T16:31:45.650603Z",
     "end_time": "2023-05-08T16:31:49.241618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Po7rTfQswAMT"
   },
   "source": [
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/hparams_parallel_coordinates.png?raw=1\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlDz2oXBgnZ9"
   },
   "source": [
    "You can try out the different views in the HParams dashboard. \n",
    "\n",
    "For example, by going to the parallel coordinates view and clicking and dragging on the accuracy axis, you can select the runs with the highest accuracy. As these runs pass through 'adam' in the optimizer axis, you can conclude that 'adam' performed better than 'sgd' on these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T16:21:02.410447Z",
     "end_time": "2023-05-08T16:21:02.413947Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hyperparameter_tuning_with_hparams.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
